{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Analysis of Kaggle 2021 Survey Participants\n",
    "\n",
    "## Introduction\n",
    "This analysis identifies three clusters of 2021 Kaggle survey participants by **k-means clustering** method. By digging further of each cluster, we name them as **explorers**, **climbers** and **experts**, depending on how they respond questions in regards of demographics, professions, their skill and knowledge in data science, the tools they are frequently using and tools they plan to get more familiar in the next two years.\n",
    "\n",
    "\n",
    "## Motivation\n",
    "This analysis tries to answer -\n",
    "\n",
    "1. How many types of professionists in the data science field?\n",
    "2. How does each segment of data science professionsts differ in demographics, professions, their skill and knowledge in data science, the tools they are frequently using and tools they plan to get more familiar in the next two years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import pyarrow\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# functions\n",
    "def rename_columns(df):\n",
    "    \"\"\"\n",
    "    input: the dataset we want to rename the columns\n",
    "    output: combine the first row of the dataset into the original column\n",
    "    \"\"\"\n",
    "    original_columns = df.columns\n",
    "    num_col = df.shape[1]\n",
    "    first_row = df.iloc[0] #grab the first row for the header\n",
    "    df = df[1:] #take the data below the first row\n",
    "    # create a list containing new column names\n",
    "    new_cols = []\n",
    "    for col in range(num_col):\n",
    "        new_col_name = original_columns[col] + '_' + first_row[col]\n",
    "        new_cols.append(new_col_name)\n",
    "    df.columns = new_cols # assign the new column names to the dataset\n",
    "    return df\n",
    "\n",
    "def replace_nan(df):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df - the target dataset\n",
    "    \n",
    "    output:\n",
    "    a new dataset with nan values replaced as 0 and non-nan values replaced with 1\n",
    "    \"\"\"\n",
    "    array = np.where(df.isnull(),0,1)\n",
    "    df = pd.DataFrame(data=array, columns=df.columns)\n",
    "    df.index = df.index + 1\n",
    "    return df\n",
    "\n",
    "def split_cols(df):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df - target dataframe\n",
    "    \n",
    "    output:\n",
    "    single_questions - a list of column names that belong to single question column\n",
    "    multiple_questions - a list of column names that belong to multiple question column\n",
    "    \"\"\"\n",
    "    single_questions = []\n",
    "    multiple_questions = []\n",
    "    for col in df.columns:\n",
    "        if 'part' in col.lower() or 'other' in col.lower():\n",
    "            multiple_questions.append(col)\n",
    "        else:\n",
    "            single_questions.append(col)\n",
    "    return single_questions, multiple_questions\n",
    "\n",
    "def pivot_col(df, col):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df - target dataset\n",
    "    col - the column we want to pivot its value as new columns\n",
    "    \n",
    "    output:\n",
    "    return a pivoted dataframe where columns are value from the col of old dataframe\n",
    "    \"\"\"\n",
    "    df['participant_id'] = df.index\n",
    "    pivoted_df = df.pivot(index = 'participant_id', columns=col, values=col).reset_index().iloc[: , 1:]\n",
    "    pivoted_df.index = pivoted_df.index + 1\n",
    "    return pivoted_df\n",
    "\n",
    "def pivot_df(df):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df - targer dataframe\n",
    "    var_cols - a list of column names we want to pivot\n",
    "    aggr - the column we used to group by the dataset\n",
    "        \n",
    "    output:\n",
    "    return a dataframe where each column comes from value of each col of old dataframe\n",
    "    NaN value replaced with 0 while non-NaN value replaced with 1\n",
    "    \"\"\"\n",
    "    \n",
    "    pivoted_df = []\n",
    "    for col in df.columns:\n",
    "        if col in single_questions: ## single question answers\n",
    "            pivoted = pivot_col(df[[col]], col)\n",
    "            pivoted_df.append(pivoted)\n",
    "        else:\n",
    "            pivoted_df.append(df[[col]])\n",
    "    pivoted_merged_df = pd.concat(pivoted_df, axis=1, ignore_index=False)\n",
    "    return pivoted_merged_df\n",
    "\n",
    "def closest_participant(participant_id, participant_matrix):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    participant_id - target participant\n",
    "    participant_matrix - matrix where shows the similarity between each participant\n",
    "    \n",
    "    output - the list of participants other than the target participant, ranked by similarity\n",
    "    \"\"\"\n",
    "    participant_list = participant_matrix[[participant_id]]\n",
    "    participant_list = participant_list.sort_values(by = participant_id, ascending = False)\n",
    "    \n",
    "    return participant_list.index[1:]\n",
    "\n",
    "def compensation(df , participant):\n",
    "    \"\"\"\n",
    "    input -\n",
    "    df - target dataset\n",
    "    participant - the id of the participant\n",
    "    \n",
    "    output -\n",
    "    the yearly compensation of that participant\n",
    "    \"\"\"\n",
    "    \n",
    "    compensation = df.loc[df.index == participant]['Q25_What is your current yearly compensation (approximate $USD)?'].iloc[0]\n",
    "    \n",
    "    return compensation\n",
    "\n",
    "def similar_user_compensation(df, participant_ids):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df - target dataset\n",
    "    participant_ids - a list of participant ids\n",
    "    output:\n",
    "    the first participant id that has non-null compensation data\n",
    "    \"\"\"\n",
    "    for participant in participant_ids:\n",
    "        if compensation(df, participant) is not None:\n",
    "            return compensation(df, participant)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "def same_answers(df, user_1, user_2):\n",
    "    \"\"\"\n",
    "    input\n",
    "    df - target dataset\n",
    "    user_1 - index number of user 1\n",
    "    user_2 - index number of user 2\n",
    "    \n",
    "    output\n",
    "    same_cols - return the column names where answer are same between user 1 and 2\n",
    "    different_cols - return the column names where answer are different between user 1 and 2\n",
    "    \"\"\"\n",
    "    same_cols = []\n",
    "    different_cols = []\n",
    "    answers = df.loc[df.index.isin([user_1,user_2])]\n",
    "    for col in answers.columns:\n",
    "        if answers[col].iloc[0] == answers[col].iloc[1]:\n",
    "            same_cols.append(col)\n",
    "        else:\n",
    "            different_cols.append(col)\n",
    "    return same_cols, different_cols\n",
    "\n",
    "def compute_correlation(df, user1, user2):\n",
    "    '''\n",
    "    INPUT\n",
    "    user1 - int user_id\n",
    "    user2 - int user_id\n",
    "    df - dataset where is a matrix of user and their pivoted answer columns\n",
    "    OUTPUT\n",
    "    the correlation between the matching ratings between the two users\n",
    "    '''\n",
    "    answer_1 = list(df.loc[df.index == user1].iloc[0])\n",
    "    answer_2 = list(df.loc[df.index == user2].iloc[0])\n",
    "    \n",
    "    dot_product = np.vdot(answer_1, answer_2)\n",
    "    \n",
    "    return dot_product #return the correlation\n",
    "\n",
    "def subset_data(df, col, criteria):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df: the dataset we want to subset from\n",
    "    col: target columns as the filter\n",
    "    criteria: value to feed the filter\n",
    "    \n",
    "    output:\n",
    "    a new dataset which is a subset of the original one\n",
    "    \"\"\"\n",
    "    \n",
    "    new_df = df.loc[df[col] == criteria]\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def question_columns(df, query, method = 'strict'):\n",
    "    \"\"\"\n",
    "    input: \n",
    "    df - target dataset\n",
    "    query - str, query we want to find relevant infomation in the dataset. e.g. 'Q7', or 'machine learning' \n",
    "    \n",
    "    output:\n",
    "    a subset of data which include the columns of the query in interest\n",
    "    \n",
    "    method:\n",
    "    if it == strict, which means we will look for the question exactly EQUALS to the query. e.g. if we search 'age', then 'language' won't\n",
    "    be taken into account in this case;\n",
    "    \n",
    "    if it == loose, which means we will look for the question exactly CONTAINS the query. e.g. if we search 'age', then 'language' will\n",
    "    be taken into account in this case.\n",
    "    \"\"\"\n",
    "    columns = df.columns\n",
    "    question_col = []\n",
    "    for col in columns:\n",
    "        if method == 'strict':\n",
    "            col_parts = col.lower().split() # each column name will be separated into single word tokens at first\n",
    "            if query.lower() in col_parts:\n",
    "                question_col.append(col)\n",
    "        elif method == 'loose':\n",
    "            if query.lower() in col.lower():\n",
    "                question_col.append(col)\n",
    "    return df[question_col]\n",
    "\n",
    "def kmeans_cluster_opt(df, init = 'k-means++', max_num_cluster = 9):\n",
    "    \"\"\"\n",
    "    input: \n",
    "    df - the dataset we want to segments into cluster\n",
    "    init - the way we want to initialize the starting centroid\n",
    "    max_num_cluster - the max number of cluster\n",
    "    \n",
    "    output:\n",
    "    a visualization showing the line graph indicating the optimal number of klusters, based on inertias value\n",
    "    \"\"\"\n",
    "    num_clusters = list(range(1, max_num_cluster))\n",
    "    inertias = []\n",
    "\n",
    "    for k in num_clusters:\n",
    "        model = KMeans(init=init, n_clusters=k, random_state = 42)\n",
    "        model.fit(df)\n",
    "        inertias.append(model.inertia_)\n",
    "\n",
    "    \n",
    "    plt.plot(num_clusters, inertias, '-o')\n",
    "\n",
    "    plt.xlabel('number of clusters (k)')\n",
    "    plt.ylabel('inertia')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def kmeans_predict(df, init = 'k-means++', n_clusters = 4):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df - dataset we want to segment into clusters\n",
    "    init - the way we want to initialize the starting centroid\n",
    "    n_clusters - the number of cluster\n",
    "    \n",
    "    output:\n",
    "    labels - return an array of predictions on the cluster label of given features\n",
    "    centers - centroid values of each cluster\n",
    "    \"\"\"\n",
    "    model = KMeans(init=init, n_clusters = n_clusters, random_state = 42)\n",
    "\n",
    "    model.fit(df)\n",
    "\n",
    "    labels = model.predict(df)\n",
    "    \n",
    "    centers = np.array(model.cluster_centers_)\n",
    "    \n",
    "    return labels, centers\n",
    "\n",
    "def percentage_row(df):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df - target dataframe\n",
    "    \n",
    "    output - a new dataframe in which each cell represents the row \n",
    "    percengatge value of the corresponding one in the target dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    new_df = df.div(df.sum(axis=1), axis=0)\n",
    "    new_df_share = round(new_df.apply(lambda x: x*100), 1).reset_index()\n",
    "    return new_df_share\n",
    "\n",
    "def cluster_aggr(df, cols):\n",
    "    \"\"\"\n",
    "    input\n",
    "    df: target dataset\n",
    "    \n",
    "    cols: columns of the question we are interested to see the segmentation\n",
    "    \n",
    "    output:\n",
    "    a new dataframe that contains the number of participants for each question option\n",
    "    \"\"\"\n",
    "    aggr = df.groupby(['cluster']).sum()\n",
    "    aggr_col = aggr.iloc[:, cols]\n",
    "    aggr_col = aggr_col.loc[:, (aggr_col != 0).any(axis=0)]\n",
    "    aggr_col.loc[\"Total\"] = aggr_col.sum()\n",
    "\n",
    "    \n",
    "    return aggr_col\n",
    "\n",
    "def plot_bar_perc(df, cols):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df - target dataframe\n",
    "    cols - columns we want to present as bars in the outcome chart\n",
    "    \n",
    "    output:\n",
    "    a bar chart where each bar represents the share of each value in the column aggregated by cluster\n",
    "    \"\"\"\n",
    "    fig = make_subplots(rows=1, cols=3, \n",
    "                    start_cell=\"bottom-left\", \n",
    "                        shared_yaxes=True,\n",
    "                    subplot_titles=(cluster_title))\n",
    "\n",
    "    clusters = df.index.tolist()\n",
    "\n",
    "    options = df.columns[1:]\n",
    "\n",
    "    colors = single_blue * len(options)\n",
    "\n",
    "    for c in range(len(clusters)):\n",
    "        data = df.loc[df['cluster'] == clusters[c]] \n",
    "        for o in range(len(options)):\n",
    "            fig.add_trace(go.Bar(x=[options[o]], \n",
    "                             y=data[options[o]],\n",
    "                             marker_color = colors[o]),\n",
    "                              row=1, col=c+1)\n",
    "        \n",
    "    fig.update_layout(\n",
    "        title=\"% of each option chosen by participants per cluster\",\n",
    "        yaxis_title=\"% of participants\",\n",
    "        showlegend=False)\n",
    "    fig.show()\n",
    "    \n",
    "def cluster_question_plot(df, question):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df - target dataset\n",
    "    \n",
    "    question - the question we are interested to segmented by the cluster\n",
    "    \n",
    "    output:\n",
    "    a list which contains a table and a plot showing the share of each segment per cluster\n",
    "    \"\"\"\n",
    "    aggr_data = cluster_aggr(df, range(qs_num[question][0], qs_num[question][1]))\n",
    "    aggr_perc = percentage_row(aggr_data)\n",
    "    plot_data = aggr_perc.loc[aggr_perc['cluster'].isin([0,1,2])]\n",
    "    \n",
    "    plot_chart = plot_bar_perc(plot_data, plot_data.columns[1:])\n",
    "    \n",
    "    return aggr_perc, plot_chart\n",
    "\n",
    "def plot_bar_rank(df, cols, num_col = 10):\n",
    "    \"\"\"\n",
    "    input\n",
    "    df: target dataframe\n",
    "    cols : Question you want to aggregate\n",
    "    num_col: number of options shown in the chart\n",
    "    \n",
    "    output:\n",
    "    return a bar chart where options with highest total shares are set at the left side\n",
    "    \"\"\"\n",
    "    data_aggr = cluster_aggr(df, range(qs_num[cols][0], qs_num[cols][1]))\n",
    "    data_aggr = percentage_row(data_aggr)\n",
    "    data_aggr_rank = rank_total(data_aggr)\n",
    "\n",
    "    top_data_aggr_rank_cols = ['cluster']\n",
    "    for col in data_aggr_rank.columns:\n",
    "        top_data_aggr_rank_cols.append(col)\n",
    "    \n",
    "    aggr_cols = data_aggr.columns\n",
    "\n",
    "    data_aggr = data_aggr.loc[data_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "    plot_bar_perc(data_aggr[top_data_aggr_rank_cols[:num_col]], aggr_cols)\n",
    "\n",
    "def std_cluster(df):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df - target dataframe\n",
    "    output:\n",
    "    std - standard deviation of each row per cluster\n",
    "    \"\"\"\n",
    "    std = df.iloc[:,1:].std(axis=1)\n",
    "    return std\n",
    "\n",
    "def rank_total(df):\n",
    "    \"\"\"\n",
    "    input: target dataframe\n",
    "    \n",
    "    output: a new dataframe which columns are ranked by the value in the Total row, so higher values are set at the left side\n",
    "    \"\"\"\n",
    "    df = df.iloc[:,1:] # remove cluster column\n",
    "    df_ranked = df.sort_values(by = 3, axis=1 , ascending = False)\n",
    "    return df_ranked\n",
    "\n",
    "def find_correlation_rank(df,col,ascending = False):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df - target dataframe\n",
    "    col - column in interest\n",
    "    \n",
    "    output:\n",
    "    a list of columns in which the highest positive correlated col ranks the first\n",
    "    \"\"\"\n",
    "    df_ranked = df[[col]].sort_values(by = col,ascending = ascending)\n",
    "    \n",
    "    return df_ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color palatte for visualization\n",
    "shades_blue = ['#90EE90','#00FF7F','#00FFFF','#89CFF0','#1434A4','#0096FF',\n",
    "               '#6495ED','#1F51FF','#2F4F4F','#A7C7E7','#00008B']\n",
    "\n",
    "single_blue = ['#89CFF0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "The dataset is from [Kaggle Machine Learning & Data Science Survey](https://www.kaggle.com/competitions/kaggle-survey-2021). According to the competition host, it has collected 25,973 valid answers from Kaggle users. Kaggle is a free online data science community where participants could attend data science competitions. Its annual survey is representative to understand professionists in the data science world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 25974 rows.\n",
      "The dataset has 368 columns.\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "data = pd.read_parquet(\"/Users/jasmine/udacity_ds_nano_degree_capstone/data.parquet\")\n",
    "\n",
    "# first five rows\n",
    "# data.head()\n",
    "\n",
    "# remove the column Time from Start to Finish (seconds)\n",
    "data = data.iloc[: , 1:]\n",
    "\n",
    "# size of the dataset\n",
    "data.shape # 25973 rows, 369 columns\n",
    "\n",
    "print(\"The dataset has \" + str(data.shape[0]) + \" rows.\")\n",
    "\n",
    "print(\"The dataset has \" + str(data.shape[1]) + \" columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-aef51b51be30>:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['participant_id'] = df.index\n"
     ]
    }
   ],
   "source": [
    "# make question as columne names\n",
    "renamed_data = rename_columns(data)\n",
    "\n",
    "# group questions into two categories\n",
    "# single_questions if it is a single answer question\n",
    "# multiple_questions if it is a multiple answer question\n",
    "single_questions = split_cols(renamed_data)[0]\n",
    "multiple_questions = split_cols(renamed_data)[1]\n",
    "\n",
    "# replace values for simplification\n",
    "renamed_data = renamed_data.replace(\"Prefer to self-describe\", \"self-describe\")\n",
    "renamed_data = renamed_data.replace(\"United Kingdom of Great Britain and Northern Ireland\", \"UK\")\n",
    "renamed_data = renamed_data.replace(\"United States of America\", \"the U.S.\")\n",
    "\n",
    "\n",
    "# pivot the dataset to one option one column\n",
    "# turn answer as binary data where chosen is 1 and not chosen is 0 \n",
    "pivoted_data = pivot_df(renamed_data)\n",
    "binary_data = replace_nan(pivoted_data)\n",
    "\n",
    "# Question and its number of columns\n",
    "qs_num = {\n",
    "    \"Age\" : [0,11],\n",
    "    \"Gender\" : [11,16],\n",
    "    \"Country\" : [16,82],\n",
    "    \"HighEdu\" : [82,89],\n",
    "    \"Employment\" : [89,104],\n",
    "    \"CodeExp\" : [104,111],\n",
    "    \"ProgLangReg\" : [111,125],\n",
    "    \"ProgLangRec\" : [125,138],\n",
    "    \"IDE\" : [138,151],\n",
    "    \"HostNotebook\" : [151, 169],\n",
    "    \"CompPlatMost\" : [169,175],\n",
    "    \"HardwareReg\" : [175,182],\n",
    "    \"TPUtimes\" : [182, 187],\n",
    "    \"VisualLib\" : [187,200],\n",
    "    \"MLmethd\" : [200,209],\n",
    "    \"MLframe\" : [209,227],\n",
    "    \"MLalgorithm\" : [227,239],\n",
    "    \"CompVis\" : [239,246],\n",
    "    \"NLP\" : [246,253],\n",
    "    \"Industry\" : [253,272],\n",
    "    \"SizeEmployer\" : [272,278],\n",
    "    \"SizeDS\" : [278,287],\n",
    "    \"DSBusiness\" : [287,292],\n",
    "    \"WorkAct\" : [292,301],\n",
    "    \"Compensation\" : [301,328],\n",
    "    \"InvestDS\" : [328, 334],\n",
    "    \"CldCompPltReg\" : [334,347],\n",
    "    \"CldCompPltBstExp\" : [347,360],\n",
    "    \"CldCompProdReg\" : [360,365],\n",
    "    \"DataStoreProdReg\" : [365,373],\n",
    "    \"ManageMLProdReg\" : [373,383],\n",
    "    \"BigDataProdReg\" : [383,405],\n",
    "    \"BigDataProdMost\" : [405,425],\n",
    "    \"IntegenceReg\" : [425,443],\n",
    "    \"IntegenceMost\" : [443,459],\n",
    "    \"IsAutoML\" : [459,467],\n",
    "    \"AutoMLReg\" : [467,475],\n",
    "    \"MLexperiment\" : [475,487],\n",
    "    \"PubShare\" : [487,497],\n",
    "    \"Courses\" : [497,510],\n",
    "    \"PrimaryTool\" : [510,516],\n",
    "    \"FavMedia\" : [516,528],\n",
    "    \"CldCompPltNxt\" : [528,540],\n",
    "    \"CldCompProdNxt\" : [540,545],\n",
    "    \"DataStoreProdNxt\" : [545,553],\n",
    "    \"ManageMLProdNxt\" : [553,563],\n",
    "    \"BigDataProdNxt\" : [563,584],\n",
    "    \"IntegenceNxt\" : [584,601],\n",
    "    \"AutoMLCatNxt\" : [601,609],\n",
    "    \"AutoMLProdNxt\" : [609,617],\n",
    "    \"MLexperimentNxt\" : [617,629]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "This analysis adopts K-means to find participant clusters based on the pattern how they respond to the survey. It  aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster [link](https://en.wikipedia.org/wiki/K-means_clustering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore how many number cluster can give a small enough inertia and also be as small number as possible\n",
    "kmeans_cluster_opt(binary_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart shows the number 3 is an \"elbow\" at the line chart, which means segmenting the participants into three clusters could return us a good number of groups and a low error as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column cluster segmenting participants\n",
    "# As the chart indicates above, we choose to make 3 clusters\n",
    "binary_data['cluster'] = kmeans_predict(binary_data , n_clusters = 3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for visualization use later\n",
    "# cluster 0 are explorers\n",
    "# cluster 1 are climbers\n",
    "# cluster 2 are experts\n",
    "cluster_title = [\"explorers\", \"climbers\", \"experts\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "First of all, let's take a look at how many participants per each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_data.groupby(['cluster']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 11,301 explorers, 6,945 climbers and 7,727 experts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_aggr = cluster_aggr(binary_data, range(qs_num[\"Age\"][0], qs_num[\"Age\"][1]))\n",
    "age_aggr = percentage_row(age_aggr)\n",
    "\n",
    "cols = age_aggr.columns\n",
    "\n",
    "cols = ['18-21','22-24','25-29','30-34','35-39','40-44','45-49','50-54','55-59','60-69','70+']\n",
    "\n",
    "age_aggr = age_aggr.loc[age_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(age_aggr, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C0 and C1 both have highest shares in the age group between 18 to 21, taking 24.9% and 25.4% respectively.\n",
    "As the age group increases, the share drops.\n",
    "\n",
    "C2 The highest share age group is 25-29, followed by 30-34 and 35-39, which indicates the C2 participants are more likely older than C0 and C1 cluster participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"Gender\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three clusters are male dominated. C2 has a relatively even higher share than C0 and C1 by over 7 percentage points. Correspondingly, the share of female participants of C2 is lower than 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"Country\",15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 participants are more likely to reside in developed countries. C0 has slight more number of participants than C1 residing in developed regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectedly, C2 don't have clearly more participants than any the other cluster in the BRICS countries, apart from Brazil. India is the biggest country that has the highest share of C1 cluster participants (35%), almost 5 times as the number as of the United States."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_aggr = cluster_aggr(binary_data, range(qs_num[\"HighEdu\"][0], qs_num[\"HighEdu\"][1]))\n",
    "edu_aggr = percentage_row(edu_aggr)\n",
    "\n",
    "cols = edu_aggr.columns.tolist()\n",
    "\n",
    "cols = [\n",
    " 'cluster',\n",
    " 'No formal education past high school',\n",
    " 'Some college/university study without earning a bachelor’s degree',\n",
    " 'Bachelor’s degree',\n",
    " 'Master’s degree',\n",
    " 'Professional doctorate',\n",
    " 'Doctoral degree',\n",
    " 'I prefer not to answer'\n",
    " ]\n",
    "\n",
    "edu_aggr = edu_aggr[cols]\n",
    "\n",
    "edu_aggr = edu_aggr.loc[edu_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(edu_aggr, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 has the highest share of participants that will have Master’s degree, Professional doctorate\tor Doctoral degree as the highest education level in the next 2 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"Employment\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing C2 with C0 and C1 clusers, C2 has the highest, almost 4 times as any of the rest, share (~29.1%) of participants who are data scientists. On the other hand, C1 and 0 have highest shares of Unemployed and Students. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"Industry\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computer and Technology is the most popular industry for all of the clusters, C2 has the highest among the three, 5% and 8 percentage points higher than C1 and C0 respectively. On the other hand, the second most popular industry is Academia/Education. Even though cluster 2 has the highest share of master and doctor degree participants, its share that work in the Academia is comparatively lower. The third most popular industry is Finance/Accounting and the share of each cluster is actually close. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_emp_aggr = cluster_aggr(binary_data, range(qs_num[\"SizeEmployer\"][0], qs_num[\"SizeEmployer\"][1]))\n",
    "size_emp_aggr = percentage_row(size_emp_aggr)\n",
    "\n",
    "cols = size_emp_aggr.columns.tolist()\n",
    "\n",
    "cols = ['cluster',\n",
    " '0-49 employees',\n",
    " '50-249 employees',\n",
    " '250-999 employees',\n",
    " '1000-9,999 employees',\n",
    " '10,000 or more employees']\n",
    "\n",
    "size_emp_aggr = size_emp_aggr[cols]\n",
    "\n",
    "size_emp_aggr = size_emp_aggr.loc[size_emp_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(size_emp_aggr, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small size comannies take the highest share of every cluster, whereas C2 participants are more likely to work in big companies than C0 and C1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_ds_aggr = cluster_aggr(binary_data, range(qs_num[\"SizeDS\"][0], qs_num[\"SizeDS\"][1]))\n",
    "size_ds_aggr = percentage_row(size_ds_aggr)\n",
    "\n",
    "cols = size_ds_aggr.columns.tolist()\n",
    "\n",
    "cols = ['cluster',\n",
    " '0','1-2','3-4','5-9','10-14','15-19','20+','I do not know']\n",
    "\n",
    "size_ds_aggr = size_ds_aggr[cols]\n",
    "\n",
    "size_ds_aggr = size_ds_aggr.loc[size_ds_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(size_ds_aggr, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No surprise, C2 cluster has the higher share of data science colleagues at their employers than any other cluster. Over 40% of participants in either C0 or C1 have no idea how many people working in data scient or they claim none actually works at this field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_business_aggr = cluster_aggr(binary_data, range(qs_num[\"DSBusiness\"][0], qs_num[\"DSBusiness\"][1]))\n",
    "ds_business_aggr = percentage_row(ds_business_aggr)\n",
    "\n",
    "cols = ds_business_aggr.columns.tolist()\n",
    "\n",
    "cols = ['cluster',\n",
    " 'No (we do not use ML methods)',\n",
    " 'We use ML methods for generating insights (but do not put working models into production)',\n",
    " 'We are exploring ML methods (and may one day put a model into production)',\n",
    " 'We recently started using ML methods (i.e., models in production for less than 2 years)',\n",
    " 'We have well established ML methods (i.e., models in production for more than 2 years)']\n",
    "\n",
    "ds_business_aggr = ds_business_aggr[cols]\n",
    "\n",
    "ds_business_aggr = ds_business_aggr.loc[ds_business_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(ds_business_aggr, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a higher share of data science team, it's no doubt that C2 has also the highest share of participants that their employer deployed or adopted ML methods in the business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"WorkAct\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a strong background as data scientists in Cluster 2, it explains why they have higher shares of participants working at ML related activites. \n",
    "\n",
    "- Build prototypes to explore applying machine learning to new areas\n",
    "\n",
    "- Build and/or run a machine learning service that operationally improves my product or workflows\n",
    "\n",
    "- Experimentation and iteration to improve existing ML models\n",
    "\n",
    "- Do research that advances the state of the art of machine learning\n",
    "\n",
    "Interestingly, C0 has a higher share of participants working at \n",
    "\n",
    "- Analyze and understand data to influence product or business decisions\n",
    "\n",
    "- None of these activities are an important part of my role at work\n",
    "\n",
    "- Other\n",
    "\n",
    "It could be they have a relatively higher participants working as business analyst / data analysts or other fields. They work closely with Data Scientists and have strong interest in this area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compensation_aggr = cluster_aggr(binary_data, range(qs_num[\"Compensation\"][0], qs_num[\"Compensation\"][1]))\n",
    "compensation_aggr = percentage_row(compensation_aggr)\n",
    "\n",
    "cols = compensation_aggr.columns.tolist()\n",
    "cols = ['cluster',\n",
    " '$0-999',\n",
    " '1,000-1,999',\n",
    " '2,000-2,999',\n",
    " '3,000-3,999',\n",
    " '4,000-4,999',\n",
    " '5,000-7,499',\n",
    " '7,500-9,999',\n",
    " '10,000-14,999',\n",
    " '15,000-19,999',\n",
    " '20,000-24,999',\n",
    " '25,000-29,999',\n",
    " '30,000-39,999',\n",
    " '40,000-49,999',\n",
    " '50,000-59,999',\n",
    " '60,000-69,999',\n",
    " '70,000-79,999',\n",
    " '80,000-89,999',\n",
    " '90,000-99,999',\n",
    " '100,000-124,999',\n",
    " '125,000-149,999',\n",
    " '150,000-199,999',\n",
    " '200,000-249,999',\n",
    " '250,000-299,999',\n",
    " '300,000-499,999',\n",
    " '$500,000-999,999',\n",
    " '>$1,000,000']\n",
    "\n",
    "compensation_aggr = compensation_aggr[cols]\n",
    "\n",
    "compensation_aggr = compensation_aggr.loc[compensation_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(compensation_aggr, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 has higher percentage of participants earning at the higher end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invest_ds_aggr = cluster_aggr(binary_data, range(qs_num[\"InvestDS\"][0], qs_num[\"InvestDS\"][1]))\n",
    "invest_ds_aggr = percentage_row(invest_ds_aggr)\n",
    "\n",
    "cols = invest_ds_aggr.columns.tolist()\n",
    "cols = ['cluster',\n",
    " '$0 ($USD)',\n",
    " '$1-$99',\n",
    " '$100-$999',\n",
    " '$1000-$9,999',\n",
    " '$10,000-$99,999',\n",
    " '$100,000 or more ($USD)']\n",
    "\n",
    "invest_ds_aggr = invest_ds_aggr[cols]\n",
    "\n",
    "invest_ds_aggr = invest_ds_aggr.loc[invest_ds_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(invest_ds_aggr, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 has intested a lot more than any other cluster. C1 has the lowest, could be they are most likely students or unemployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codeExp_aggr = cluster_aggr(binary_data, range(qs_num[\"CodeExp\"][0], qs_num[\"CodeExp\"][1]))\n",
    "codeExp_aggr = percentage_row(codeExp_aggr)\n",
    "\n",
    "cols = codeExp_aggr.columns.tolist()\n",
    "cols = ['cluster',\n",
    " 'I have never written code',\n",
    " '< 1 years',\n",
    " '1-3 years',\n",
    " '3-5 years',\n",
    " '5-10 years',\n",
    " '10-20 years',\n",
    " '20+ years']\n",
    "\n",
    "codeExp_aggr = codeExp_aggr[cols]\n",
    "\n",
    "codeExp_aggr = codeExp_aggr.loc[codeExp_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(codeExp_aggr, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 participants have coding experience more or less, it has the highest percentages of participants coding for over 5 years. On the other hand, C1 have fewer share of participants have no coding experience than the C0 and has a relatively higher share than the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"ProgLangReg\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is the most popular regular programming language, followed by SQL. The third regular programming languages differ that Cluster 2 favors R more, while C1 and C0 prefers C++. It could be the higher share of DS participants allocated in C2 and they are consumers of the R language for statistical modeling. On the other hand, as the role question suggested, software engineers take a noticeable share in C0 and C1 and they use C++ as regular language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"ProgLangRec\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No doubtedly, Python is the most popular language in the recommendation. But interestingly, even though C0 participants are the least group using R as regular language, they suggest it as the most recommending one, which indicates they are aware that this language is a must if you want to understand more of what data scientists do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLmethd_aggr = cluster_aggr(binary_data, range(qs_num[\"MLmethd\"][0], qs_num[\"MLmethd\"][1]))\n",
    "MLmethd_aggr = percentage_row(MLmethd_aggr)\n",
    "\n",
    "cols = MLmethd_aggr.columns.tolist()\n",
    "cols = ['cluster',\n",
    " 'I do not use machine learning methods',\n",
    " 'Under 1 year',\n",
    " '1-2 years',\n",
    " '2-3 years',\n",
    " '3-4 years',\n",
    " '4-5 years',\n",
    " '5-10 years',\n",
    " '10-20 years',\n",
    " '20 or more years']\n",
    "\n",
    "MLmethd_aggr = MLmethd_aggr[cols]\n",
    "\n",
    "MLmethd_aggr = MLmethd_aggr.loc[MLmethd_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(MLmethd_aggr, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C0 has a lot more participants who don't use ML methods at all, 3 times and 30 times as C1 and C2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"VisualLib\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 cluster has a wider option in visualization tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"MLframe\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 cluster has a wider option in ML framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"MLalgorithm\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 cluster has a wider option in ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"CompVis\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1 and C2 have similar level of computer vision interests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"NLP\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 and C1 have far more shares of participants using NLP methods than C0. C2 has clearly more than C1 in Contextualized embeddings (ELMo, CoVe) and Transformer language models (GPT-3, BERT, XLnet, etc) which are less popular methods overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"PubShare\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 has a very high willing to share their work and knowledge publicly. However, the are a bit less likely to share on Kaggle than C1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"Courses\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1 and C2 are more likely to choose online courses than C0. Compared with C1, C2 participants are less into Kaggle courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"FavMedia\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1 and C2 are more likely to choose data science media than C0. C2 participants are more into Email, newsletter, podcasts and journals. C0 and C1 are more into Youtube and Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"IDE\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the main difference among each IDE? - Maybe closely with the knowledge and skillset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"HostNotebook\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "company (work) related?\n",
    "\n",
    "needs to figure out the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"CompPlatMost\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 have more in cloud computing platform and deep learning workstation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"HardwareReg\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 have more in NVIDIA GPU and Google Cloud TPU than C1 or C0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPUtimes_aggr = cluster_aggr(binary_data, range(qs_num[\"TPUtimes\"][0], qs_num[\"TPUtimes\"][1]))\n",
    "TPUtimes_aggr = percentage_row(TPUtimes_aggr)\n",
    "\n",
    "cols = TPUtimes_aggr.columns.tolist()\n",
    "cols = ['cluster',\n",
    "        'Never',\n",
    "        'Once',\n",
    "        '2-5 times',\n",
    "        '6-25 times',\n",
    "        'More than 25 times']\n",
    "\n",
    "TPUtimes_aggr = TPUtimes_aggr[cols]\n",
    "\n",
    "TPUtimes_aggr = TPUtimes_aggr.loc[TPUtimes_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(TPUtimes_aggr, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More times of C2 than C0 or C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"CldCompPltReg\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS,Micosoft Azure, and GCP - C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"CldCompPltBstExp\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"CldCompProdReg\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"DataStoreProdReg\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"ManageMLProdReg\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"BigDataProdReg\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 has more and wider options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"BigDataProdMost\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks C1 is more centralized in the options of most big data products. C2 has more in postgress while C0 has more in microsoft SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"CldCompProdReg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 has more. C0 has same level of number in Microsoft Azure Virtual Machines as C2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"IntegenceReg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 has more and wider option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"IntegenceMost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1 likes Power BI more while C0 and C2 likes Tableau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"IsAutoML\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First category that over 40% of the C2 participants chose none. \n",
    "\n",
    "That might explain why they want to learn it in the next 2 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"AutoMLReg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 has the highest share chosen None. \n",
    "\n",
    "Is this the next future?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"MLexperiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A category which is less chosen. C2 have over half participants chose 0 and over 80% for any other cluster.\n",
    "\n",
    "TensorBoard and MLflow are most popular ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"PrimaryTool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1 and C2 learned how to code at the very beginning when they start to step into the statistical world.\n",
    "\n",
    "That explains the coding experience.\n",
    "\n",
    "C0 starts with statistics software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"CldCompPltNxt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1 is the most ambicious and C2 is the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"CldCompProdNxt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1 is the most ambicious and C2 is the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data unavailable\n",
    "# plot_bar_rank(binary_data, \"DataStoreProdNxt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No data available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"ManageMLProdNxt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1 is the most ambicious and C2 is the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"BigDataProdNxt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1 is the most ambicious and C2 is the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"IntegenceNxt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1 is the most ambicious and C2 is the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"AutoMLCatNxt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "too hard for C0. C1 is the most ambicious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"AutoMLProdNxt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1 is the most ambicious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"MLexperimentNxt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1 is the most ambicious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirment\n",
    "Python libraries are required to run this notebook\n",
    "\n",
    "os/numpy/pandas/matplotlib/sklearn/pyarrow/plotly\n",
    "\n",
    "## Acknowledge\n",
    "The dataset for building this model is coming from Kaggle. Kudos to the Kaggle community!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
