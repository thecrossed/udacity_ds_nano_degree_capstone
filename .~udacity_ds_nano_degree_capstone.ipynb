{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-10T09:11:35.170766Z",
     "iopub.status.busy": "2022-07-10T09:11:35.170288Z",
     "iopub.status.idle": "2022-07-10T09:11:35.981157Z",
     "shell.execute_reply": "2022-07-10T09:11:35.979943Z",
     "shell.execute_reply.started": "2022-07-10T09:11:35.170669Z"
    }
   },
   "source": [
    "## left to do\n",
    "\n",
    "### products / platforms understanding their differences\n",
    "\n",
    "### percentage spread out statistical way\n",
    "\n",
    "#### Peakers, Climbers and Wanderers - the Data Science profession landscape\n",
    "\n",
    "#### google spreadsheet\n",
    "#### https://docs.google.com/spreadsheets/d/1ogM8BzvagSfFrYNCBkxVXeRb31UDkzy3_G-FOfVuhyY/edit?usp=sharing\n",
    "\n",
    "#### Criteria\n",
    "\n",
    "1. binary, yes/no. Choosing rate. (None or not None)\n",
    "2. Spreading/diversity/option breadth. (fluatuate bewteen each option)\n",
    "3. Order tendency. Towards the higher end or lower end of the order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Analysis of Kaggle 2021 Survey Participants\n",
    "\n",
    "## Introduction\n",
    "\n",
    "## Problem Statement\n",
    "The fundatmental question of this analysis is how many clusters regarding the statistical pattern users answer the survey differently. Based on that, I want to also understand -\n",
    "\n",
    "1. How does the participant profile looks like per cluster?\n",
    "I have grouped the survey questions into five - demograpgics, profession, knowledge and skillset, learning and development. It offers us various perspective to make the profile overall of each cluster.\n",
    "\n",
    "\n",
    "2. Is there any similarity or relationship between the participant cluster? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import pyarrow\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# functions\n",
    "def rename_columns(df):\n",
    "    \"\"\"\n",
    "    input: the dataset we want to rename the columns\n",
    "    output: combine the first row of the dataset into the original column\n",
    "    \"\"\"\n",
    "    original_columns = df.columns\n",
    "    num_col = df.shape[1]\n",
    "    first_row = df.iloc[0] #grab the first row for the header\n",
    "    df = df[1:] #take the data below the first row\n",
    "    # create a list containing new column names\n",
    "    new_cols = []\n",
    "    for col in range(num_col):\n",
    "        new_col_name = original_columns[col] + '_' + first_row[col]\n",
    "        new_cols.append(new_col_name)\n",
    "    df.columns = new_cols # assign the new column names to the dataset\n",
    "    return df\n",
    "\n",
    "def replace_nan(df):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df - the target dataset\n",
    "    \n",
    "    output:\n",
    "    a new dataset with nan values replaced as 0 and non-nan values replaced with 1\n",
    "    \"\"\"\n",
    "    array = np.where(df.isnull(),0,1)\n",
    "    df = pd.DataFrame(data=array, columns=df.columns)\n",
    "    df.index = df.index + 1\n",
    "    return df\n",
    "\n",
    "def split_cols(df):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df - target dataframe\n",
    "    \n",
    "    output:\n",
    "    single_questions - a list of column names that belong to single question column\n",
    "    multiple_questions - a list of column names that belong to multiple question column\n",
    "    \"\"\"\n",
    "    single_questions = []\n",
    "    multiple_questions = []\n",
    "    for col in df.columns:\n",
    "        if 'part' in col.lower() or 'other' in col.lower():\n",
    "            multiple_questions.append(col)\n",
    "        else:\n",
    "            single_questions.append(col)\n",
    "    return single_questions, multiple_questions\n",
    "\n",
    "def pivot_col(df, col):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df - target dataset\n",
    "    col - the column we want to pivot its value as new columns\n",
    "    \n",
    "    output:\n",
    "    return a pivoted dataframe where columns are value from the col of old dataframe\n",
    "    \"\"\"\n",
    "    df['participant_id'] = df.index\n",
    "    pivoted_df = df.pivot(index = 'participant_id', columns=col, values=col).reset_index().iloc[: , 1:]\n",
    "    pivoted_df.index = pivoted_df.index + 1\n",
    "    return pivoted_df\n",
    "\n",
    "def pivot_df(df):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df - targer dataframe\n",
    "    var_cols - a list of column names we want to pivot\n",
    "    aggr - the column we used to group by the dataset\n",
    "        \n",
    "    output:\n",
    "    return a dataframe where each column comes from value of each col of old dataframe\n",
    "    NaN value replaced with 0 while non-NaN value replaced with 1\n",
    "    \"\"\"\n",
    "    \n",
    "    pivoted_df = []\n",
    "    for col in df.columns:\n",
    "        if col in single_questions: ## single question answers\n",
    "            pivoted = pivot_col(df[[col]], col)\n",
    "            pivoted_df.append(pivoted)\n",
    "        else:\n",
    "            pivoted_df.append(df[[col]])\n",
    "    pivoted_merged_df = pd.concat(pivoted_df, axis=1, ignore_index=False)\n",
    "    return pivoted_merged_df\n",
    "\n",
    "def closest_participant(participant_id, participant_matrix):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    participant_id - target participant\n",
    "    participant_matrix - matrix where shows the similarity between each participant\n",
    "    \n",
    "    output - the list of participants other than the target participant, ranked by similarity\n",
    "    \"\"\"\n",
    "    participant_list = participant_matrix[[participant_id]]\n",
    "    participant_list = participant_list.sort_values(by = participant_id, ascending = False)\n",
    "    \n",
    "    return participant_list.index[1:]\n",
    "\n",
    "def compensation(df , participant):\n",
    "    \"\"\"\n",
    "    input -\n",
    "    df - target dataset\n",
    "    participant - the id of the participant\n",
    "    \n",
    "    output -\n",
    "    the yearly compensation of that participant\n",
    "    \"\"\"\n",
    "    \n",
    "    compensation = df.loc[df.index == participant]['Q25_What is your current yearly compensation (approximate $USD)?'].iloc[0]\n",
    "    \n",
    "    return compensation\n",
    "\n",
    "def similar_user_compensation(df, participant_ids):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df - target dataset\n",
    "    participant_ids - a list of participant ids\n",
    "    output:\n",
    "    the first participant id that has non-null compensation data\n",
    "    \"\"\"\n",
    "    for participant in participant_ids:\n",
    "        if compensation(df, participant) is not None:\n",
    "            return compensation(df, participant)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "def same_answers(df, user_1, user_2):\n",
    "    \"\"\"\n",
    "    input\n",
    "    df - target dataset\n",
    "    user_1 - index number of user 1\n",
    "    user_2 - index number of user 2\n",
    "    \n",
    "    output\n",
    "    same_cols - return the column names where answer are same between user 1 and 2\n",
    "    different_cols - return the column names where answer are different between user 1 and 2\n",
    "    \"\"\"\n",
    "    same_cols = []\n",
    "    different_cols = []\n",
    "    answers = df.loc[df.index.isin([user_1,user_2])]\n",
    "    for col in answers.columns:\n",
    "        if answers[col].iloc[0] == answers[col].iloc[1]:\n",
    "            same_cols.append(col)\n",
    "        else:\n",
    "            different_cols.append(col)\n",
    "    return same_cols, different_cols\n",
    "\n",
    "def compute_correlation(df, user1, user2):\n",
    "    '''\n",
    "    INPUT\n",
    "    user1 - int user_id\n",
    "    user2 - int user_id\n",
    "    df - dataset where is a matrix of user and their pivoted answer columns\n",
    "    OUTPUT\n",
    "    the correlation between the matching ratings between the two users\n",
    "    '''\n",
    "    answer_1 = list(df.loc[df.index == user1].iloc[0])\n",
    "    answer_2 = list(df.loc[df.index == user2].iloc[0])\n",
    "    \n",
    "    dot_product = np.vdot(answer_1, answer_2)\n",
    "    \n",
    "    return dot_product #return the correlation\n",
    "\n",
    "def subset_data(df, col, criteria):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df: the dataset we want to subset from\n",
    "    col: target columns as the filter\n",
    "    criteria: value to feed the filter\n",
    "    \n",
    "    output:\n",
    "    a new dataset which is a subset of the original one\n",
    "    \"\"\"\n",
    "    \n",
    "    new_df = df.loc[df[col] == criteria]\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def question_columns(df, query, method = 'strict'):\n",
    "    \"\"\"\n",
    "    input: \n",
    "    df - target dataset\n",
    "    query - str, query we want to find relevant infomation in the dataset. e.g. 'Q7', or 'machine learning' \n",
    "    \n",
    "    output:\n",
    "    a subset of data which include the columns of the query in interest\n",
    "    \n",
    "    method:\n",
    "    if it == strict, which means we will look for the question exactly EQUALS to the query. e.g. if we search 'age', then 'language' won't\n",
    "    be taken into account in this case;\n",
    "    \n",
    "    if it == loose, which means we will look for the question exactly CONTAINS the query. e.g. if we search 'age', then 'language' will\n",
    "    be taken into account in this case.\n",
    "    \"\"\"\n",
    "    columns = df.columns\n",
    "    question_col = []\n",
    "    for col in columns:\n",
    "        if method == 'strict':\n",
    "            col_parts = col.lower().split() # each column name will be separated into single word tokens at first\n",
    "            if query.lower() in col_parts:\n",
    "                question_col.append(col)\n",
    "        elif method == 'loose':\n",
    "            if query.lower() in col.lower():\n",
    "                question_col.append(col)\n",
    "    return df[question_col]\n",
    "\n",
    "def kmeans_cluster_opt(df, init = 'k-means++', max_num_cluster = 9):\n",
    "    \"\"\"\n",
    "    input: \n",
    "    df - the dataset we want to segments into cluster\n",
    "    init - the way we want to initialize the starting centroid\n",
    "    max_num_cluster - the max number of cluster\n",
    "    \n",
    "    output:\n",
    "    a visualization showing the line graph indicating the optimal number of klusters, based on inertias value\n",
    "    \"\"\"\n",
    "    num_clusters = list(range(1, max_num_cluster))\n",
    "    inertias = []\n",
    "\n",
    "    for k in num_clusters:\n",
    "        model = KMeans(init=init, n_clusters=k, random_state = 42)\n",
    "        model.fit(df)\n",
    "        inertias.append(model.inertia_)\n",
    "\n",
    "    \n",
    "    plt.plot(num_clusters, inertias, '-o')\n",
    "\n",
    "    plt.xlabel('number of clusters (k)')\n",
    "    plt.ylabel('inertia')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def kmeans_predict(df, init = 'k-means++', n_clusters = 4):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df - dataset we want to segment into clusters\n",
    "    init - the way we want to initialize the starting centroid\n",
    "    n_clusters - the number of cluster\n",
    "    \n",
    "    output:\n",
    "    labels - return an array of predictions on the cluster label of given features\n",
    "    centers - centroid values of each cluster\n",
    "    \"\"\"\n",
    "    model = KMeans(init=init, n_clusters = n_clusters, random_state = 42)\n",
    "\n",
    "    model.fit(df)\n",
    "\n",
    "    labels = model.predict(df)\n",
    "    \n",
    "    centers = np.array(model.cluster_centers_)\n",
    "    \n",
    "    return labels, centers\n",
    "\n",
    "def percentage_row(df):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df - target dataframe\n",
    "    \n",
    "    output - a new dataframe in which each cell represents the row \n",
    "    percengatge value of the corresponding one in the target dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    new_df = df.div(df.sum(axis=1), axis=0)\n",
    "    new_df_share = round(new_df.apply(lambda x: x*100), 1).reset_index()\n",
    "    return new_df_share\n",
    "\n",
    "def cluster_aggr(df, cols):\n",
    "    \"\"\"\n",
    "    input\n",
    "    df: target dataset\n",
    "    \n",
    "    cols: columns of the question we are interested to see the segmentation\n",
    "    \n",
    "    output:\n",
    "    a new dataframe that contains the number of participants for each question option\n",
    "    \"\"\"\n",
    "    aggr = df.groupby(['cluster']).sum()\n",
    "    aggr_col = aggr.iloc[:, cols]\n",
    "    aggr_col = aggr_col.loc[:, (aggr_col != 0).any(axis=0)]\n",
    "    aggr_col.loc[\"Total\"] = aggr_col.sum()\n",
    "\n",
    "    \n",
    "    return aggr_col\n",
    "\n",
    "def plot_bar_perc(df, cols):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df - target dataframe\n",
    "    cols - columns we want to present as bars in the outcome chart\n",
    "    \n",
    "    output:\n",
    "    a bar chart where each bar represents the share of each value in the column aggregated by cluster\n",
    "    \"\"\"\n",
    "    fig = make_subplots(rows=1, cols=3, \n",
    "                    start_cell=\"bottom-left\", \n",
    "                        shared_yaxes=True,\n",
    "                    subplot_titles=(cluster_title))\n",
    "\n",
    "    clusters = df.index.tolist()\n",
    "\n",
    "    options = df.columns[1:]\n",
    "\n",
    "    colors = single_blue * len(options)\n",
    "\n",
    "    for c in range(len(clusters)):\n",
    "        data = df.loc[df['cluster'] == clusters[c]] \n",
    "        for o in range(len(options)):\n",
    "            fig.add_trace(go.Bar(x=[options[o]], \n",
    "                             y=data[options[o]],\n",
    "                             marker_color = colors[o]),\n",
    "                              row=1, col=c+1)\n",
    "        \n",
    "    fig.update_layout(\n",
    "        title=\"% of each option chosen by participants per cluster\",\n",
    "        yaxis_title=\"% of participants\",\n",
    "        showlegend=False)\n",
    "    fig.show()\n",
    "    \n",
    "def cluster_question_plot(df, question):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df - target dataset\n",
    "    \n",
    "    question - the question we are interested to segmented by the cluster\n",
    "    \n",
    "    output:\n",
    "    a list which contains a table and a plot showing the share of each segment per cluster\n",
    "    \"\"\"\n",
    "    aggr_data = cluster_aggr(df, range(qs_num[question][0], qs_num[question][1]))\n",
    "    aggr_perc = percentage_row(aggr_data)\n",
    "    plot_data = aggr_perc.loc[aggr_perc['cluster'].isin([0,1,2])]\n",
    "    \n",
    "    plot_chart = plot_bar_perc(plot_data, plot_data.columns[1:])\n",
    "    \n",
    "    return aggr_perc, plot_chart\n",
    "\n",
    "def plot_bar_rank(df, cols, num_col = 10):\n",
    "    \"\"\"\n",
    "    input\n",
    "    df: target dataframe\n",
    "    cols : Question you want to aggregate\n",
    "    num_col: number of options shown in the chart\n",
    "    \n",
    "    output:\n",
    "    return a bar chart where options with highest total shares are set at the left side\n",
    "    \"\"\"\n",
    "    data_aggr = cluster_aggr(df, range(qs_num[cols][0], qs_num[cols][1]))\n",
    "    data_aggr = percentage_row(data_aggr)\n",
    "    data_aggr_rank = rank_total(data_aggr)\n",
    "\n",
    "    top_data_aggr_rank_cols = ['cluster']\n",
    "    for col in data_aggr_rank.columns:\n",
    "        top_data_aggr_rank_cols.append(col)\n",
    "    \n",
    "    aggr_cols = data_aggr.columns\n",
    "\n",
    "    data_aggr = data_aggr.loc[data_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "    plot_bar_perc(data_aggr[top_data_aggr_rank_cols[:num_col]], aggr_cols)\n",
    "\n",
    "def std_cluster(df):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df - target dataframe\n",
    "    output:\n",
    "    std - standard deviation of each row per cluster\n",
    "    \"\"\"\n",
    "    std = df.iloc[:,1:].std(axis=1)\n",
    "    return std\n",
    "\n",
    "def rank_total(df):\n",
    "    \"\"\"\n",
    "    input: target dataframe\n",
    "    \n",
    "    output: a new dataframe which columns are ranked by the value in the Total row, so higher values are set at the left side\n",
    "    \"\"\"\n",
    "    df = df.iloc[:,1:] # remove cluster column\n",
    "    df_ranked = df.sort_values(by = 3, axis=1 , ascending = False)\n",
    "    return df_ranked\n",
    "\n",
    "def find_correlation_rank(df,col,ascending = False):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df - target dataframe\n",
    "    col - column in interest\n",
    "    \n",
    "    output:\n",
    "    a list of columns in which the highest positive correlated col ranks the first\n",
    "    \"\"\"\n",
    "    df_ranked = df[[col]].sort_values(by = col,ascending = ascending)\n",
    "    \n",
    "    return df_ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color palatte for visualization\n",
    "shades_blue = ['#90EE90','#00FF7F','#00FFFF','#89CFF0','#1434A4','#0096FF',\n",
    "               '#6495ED','#1F51FF','#2F4F4F','#A7C7E7','#00008B']\n",
    "\n",
    "single_blue = ['#89CFF0']\n",
    "\n",
    "# Question and its number of columns\n",
    "qs_num = {\n",
    "    \"Age\" : [0,11],\n",
    "    \"Gender\" : [11,16],\n",
    "    \"Country\" : [16,82],\n",
    "    \"HighEdu\" : [82,89],\n",
    "    \"Employment\" : [89,104],\n",
    "    \"CodeExp\" : [104,111],\n",
    "    \"ProgLangReg\" : [111,125],\n",
    "    \"ProgLangRec\" : [125,138],\n",
    "    \"IDE\" : [138,151],\n",
    "    \"HostNotebook\" : [151, 169],\n",
    "    \"CompPlatMost\" : [169,175],\n",
    "    \"HardwareReg\" : [175,182],\n",
    "    \"TPUtimes\" : [182, 187],\n",
    "    \"VisualLib\" : [187,200],\n",
    "    \"MLmethd\" : [200,209],\n",
    "    \"MLframe\" : [209,227],\n",
    "    \"MLalgorithm\" : [227,239],\n",
    "    \"CompVis\" : [239,246],\n",
    "    \"NLP\" : [246,253],\n",
    "    \"Industry\" : [253,272],\n",
    "    \"SizeEmployer\" : [272,278],\n",
    "    \"SizeDS\" : [278,287],\n",
    "    \"DSBusiness\" : [287,292],\n",
    "    \"WorkAct\" : [292,301],\n",
    "    \"Compensation\" : [301,328],\n",
    "    \"InvestDS\" : [328, 334],\n",
    "    \"CldCompPltReg\" : [334,347],\n",
    "    \"CldCompPltBstExp\" : [347,360],\n",
    "    \"CldCompProdReg\" : [360,365],\n",
    "    \"DataStoreProdReg\" : [365,373],\n",
    "    \"ManageMLProdReg\" : [373,383],\n",
    "    \"BigDataProdReg\" : [383,405],\n",
    "    \"BigDataProdMost\" : [405,425],\n",
    "    \"IntegenceReg\" : [425,443],\n",
    "    \"IntegenceMost\" : [443,459],\n",
    "    \"IsAutoML\" : [459,467],\n",
    "    \"AutoMLReg\" : [467,475],\n",
    "    \"MLexperiment\" : [475,487],\n",
    "    \"PubShare\" : [487,497],\n",
    "    \"Courses\" : [497,510],\n",
    "    \"PrimaryTool\" : [510,516],\n",
    "    \"FavMedia\" : [516,528],\n",
    "    \"CldCompPltNxt\" : [528,540],\n",
    "    \"CldCompProdNxt\" : [540,545],\n",
    "    \"DataStoreProdNxt\" : [545,553],\n",
    "    \"ManageMLProdNxt\" : [553,563],\n",
    "    \"BigDataProdNxt\" : [563,584],\n",
    "    \"IntegenceNxt\" : [584,601],\n",
    "    \"AutoMLCatNxt\" : [601,609],\n",
    "    \"AutoMLProdNxt\" : [609,617],\n",
    "    \"MLexperimentNxt\" : [617,629]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "The dataset is from [Kaggle Machine Learning & Data Science Survey](https://www.kaggle.com/competitions/kaggle-survey-2021). According to the competition host, it has collected 25,973 valid answers from Kaggle users. Kaggle is a free online data science community where participants could attend data science competitions. Its annual survey is representative to understand professionists in the data science world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 25974 rows.\n",
      "The dataset has 368 columns.\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "data = pd.read_parquet(\"/Users/jasmine/udacity_ds_nano_degree_capstone/data.parquet\")\n",
    "\n",
    "# first five rows\n",
    "# data.head()\n",
    "\n",
    "# remove the column Time from Start to Finish (seconds)\n",
    "data = data.iloc[: , 1:]\n",
    "\n",
    "# size of the dataset\n",
    "data.shape # 25973 rows, 369 columns\n",
    "\n",
    "print(\"The dataset has \" + str(data.shape[0]) + \" rows.\")\n",
    "\n",
    "print(\"The dataset has \" + str(data.shape[1]) + \" columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-aef51b51be30>:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['participant_id'] = df.index\n"
     ]
    }
   ],
   "source": [
    "# make question as columne names\n",
    "renamed_data = rename_columns(data)\n",
    "\n",
    "# group questions into two categories\n",
    "# single_questions if it is a single answer question\n",
    "# multiple_questions if it is a multiple answer question\n",
    "single_questions = split_cols(renamed_data)[0]\n",
    "multiple_questions = split_cols(renamed_data)[1]\n",
    "\n",
    "# pivot the dataset to one option one column\n",
    "# turn answer as binary data where chosen is 1 and not chosen is 0 \n",
    "pivoted_data = pivot_df(renamed_data)\n",
    "binary_data = replace_nan(pivoted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "This analysis adopts K-means to find participant clusters based on the pattern how they respond to the survey. It  aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster [link](https://en.wikipedia.org/wiki/K-means_clustering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore how many number cluster can give a small enough inertia and also be as small number as possible\n",
    "kmeans_cluster_opt(binary_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart shows the number 3 is an \"elbow\" at the line chart, which means segmenting the participants into three clusters could return us a good number of groups and a low error as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column cluster segmenting participants\n",
    "# As the chart indicates above, we choose to make 3 clusters\n",
    "binary_data['cluster'] = kmeans_predict(binary_data , n_clusters = 3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for visualization use later\n",
    "cluster_title = [\"cluster 0\", \"cluster 1\", \"cluster 2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "First of all, let's take a look at how many participants per each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_data.groupby(['cluster']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_aggr = cluster_aggr(binary_data, range(qs_num[\"Age\"][0], qs_num[\"Age\"][1]))\n",
    "age_aggr = percentage_row(age_aggr)\n",
    "\n",
    "cols = age_aggr.columns\n",
    "\n",
    "age_aggr = age_aggr.loc[age_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(age_aggr, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C0 and C1 both have highest shares in the age group between 18 to 21, taking 24.9% and 25.4% respectively.\n",
    "As the age group increases, the share drops.\n",
    "\n",
    "C2 The highest share age group is 25-29, followed by 30-34 and 35-39, which indicates the C2 participants are more likely older than C0 and C1 cluster participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_aggr = cluster_aggr(binary_data, range(qs_num[\"Gender\"][0], qs_num[\"Gender\"][1]))\n",
    "gender_aggr = percentage_row(gender_aggr)\n",
    "\n",
    "cols = gender_aggr.columns\n",
    "\n",
    "gender_aggr = gender_aggr.loc[gender_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(gender_aggr, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three clusters are male dominated. C2 has a relatively even higher share than C0 and C1 by over 7 percentage points. Correspondingly, the share of female participants of C2 is lower than 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "developed_regions = ['cluster','Australia', 'United Kingdom of Great Britain and Northern Ireland',\n",
    "       'United States of America', 'Canada',  'Japan', 'Netherlands', 'South Korea', 'Spain',\n",
    "       'Sweden', 'France', 'Germany','Denmark',  'Greece', 'Hong Kong (S.A.R.)', 'Austria', 'Belgium', \n",
    "       'Ireland', 'Israel', 'Italy',  'Norway', 'Portugal', 'Singapore', 'Switzerland', 'Taiwan']\n",
    "\n",
    "BRICS = ['cluster','Brazil', 'China', 'India', 'Russia', 'South Africa'] \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_aggr = cluster_aggr(binary_data, range(qs_num[\"Country\"][0], qs_num[\"Country\"][1]))\n",
    "country_aggr = percentage_row(country_aggr)\n",
    "top_country = rank_total(country_aggr)\n",
    "\n",
    "top_regions = ['cluster']\n",
    "for col in top_country.columns[:14]:\n",
    "    top_regions.append(col)\n",
    "    \n",
    "cols = country_aggr.columns\n",
    "\n",
    "country_aggr = country_aggr.loc[country_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(country_aggr[top_regions[:14]], cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 participants are more likely to reside in developed countries. C0 has slight more number of participants than C1 residing in developed regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectedly, C2 don't have clearly more participants than any the other cluster in the BRICS countries, apart from Brazil. India is the biggest country that has the highest share of C1 cluster participants (35%), almost 5 times as the number as of the United States."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_aggr = cluster_aggr(binary_data, range(qs_num[\"HighEdu\"][0], qs_num[\"HighEdu\"][1]))\n",
    "edu_aggr = percentage_row(edu_aggr)\n",
    "\n",
    "cols = edu_aggr.columns.tolist()\n",
    "\n",
    "cols = [\n",
    " 'cluster',\n",
    " 'No formal education past high school',\n",
    " 'Some college/university study without earning a bachelor’s degree',\n",
    " 'Bachelor’s degree',\n",
    " 'Master’s degree',\n",
    " 'Professional doctorate',\n",
    " 'Doctoral degree',\n",
    " 'I prefer not to answer'\n",
    " ]\n",
    "\n",
    "edu_aggr = edu_aggr[cols]\n",
    "\n",
    "edu_aggr = edu_aggr.loc[edu_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(edu_aggr, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 has the highest share of participants that will have Master’s degree, Professional doctorate\tor Doctoral degree as the highest education level in the next 2 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employ_aggr = cluster_aggr(binary_data, range(qs_num[\"Employment\"][0], qs_num[\"Employment\"][1]))\n",
    "employ_aggr = percentage_row(employ_aggr)\n",
    "top_employ_rank = rank_total(employ_aggr)\n",
    "\n",
    "top_employ_cols = ['cluster']\n",
    "for col in top_employ_rank.columns:\n",
    "    top_employ_cols.append(col)\n",
    "    \n",
    "cols = employ_aggr.columns\n",
    "\n",
    "employ_aggr = employ_aggr.loc[employ_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(employ_aggr[top_employ_cols], cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing C2 with C0 and C1 clusers, C2 has the highest, almost 4 times as any of the rest, share (~29.1%) of participants who are data scientists. On the other hand, C1 and 0 have highest shares of Unemployed and Students. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indus_aggr = cluster_aggr(binary_data, range(qs_num[\"Industry\"][0], qs_num[\"Industry\"][1]))\n",
    "indus_aggr = percentage_row(indus_aggr)\n",
    "top_indus_rank = rank_total(indus_aggr)\n",
    "\n",
    "top_indus_cols = ['cluster']\n",
    "for col in top_indus_rank.columns:\n",
    "    top_indus_cols.append(col)\n",
    "    \n",
    "cols = indus_aggr.columns\n",
    "\n",
    "indus_aggr = indus_aggr.loc[indus_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(indus_aggr[top_indus_cols[:10]], cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computer and Technology is the most popular industry for all of the clusters, C2 has the highest among the three, 5% and 8 percentage points higher than C1 and C0 respectively. On the other hand, the second most popular industry is Academia/Education. Even though cluster 2 has the highest share of master and doctor degree participants, its share that work in the Academia is comparatively lower. The third most popular industry is Finance/Accounting and the share of each cluster is actually close. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_emp_aggr = cluster_aggr(binary_data, range(qs_num[\"SizeEmployer\"][0], qs_num[\"SizeEmployer\"][1]))\n",
    "size_emp_aggr = percentage_row(size_emp_aggr)\n",
    "\n",
    "cols = size_emp_aggr.columns.tolist()\n",
    "\n",
    "cols = ['cluster',\n",
    " '0-49 employees',\n",
    " '50-249 employees',\n",
    " '250-999 employees',\n",
    " '1000-9,999 employees',\n",
    " '10,000 or more employees']\n",
    "\n",
    "size_emp_aggr = size_emp_aggr[cols]\n",
    "\n",
    "size_emp_aggr = size_emp_aggr.loc[size_emp_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(size_emp_aggr, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small size comannies take the highest share of every cluster, whereas C2 participants are more likely to work in big companies than C0 and C1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_ds_aggr = cluster_aggr(binary_data, range(qs_num[\"SizeDS\"][0], qs_num[\"SizeDS\"][1]))\n",
    "size_ds_aggr = percentage_row(size_ds_aggr)\n",
    "\n",
    "cols = size_ds_aggr.columns.tolist()\n",
    "\n",
    "cols = ['cluster',\n",
    " '0','1-2','3-4','5-9','10-14','15-19','20+','I do not know']\n",
    "\n",
    "size_ds_aggr = size_ds_aggr[cols]\n",
    "\n",
    "size_ds_aggr = size_ds_aggr.loc[size_ds_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(size_ds_aggr, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No surprise, C2 cluster has the higher share of data science colleagues at their employers than any other cluster. Over 40% of participants in either C0 or C1 have no idea how many people working in data scient or they claim none actually works at this field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_business_aggr = cluster_aggr(binary_data, range(qs_num[\"DSBusiness\"][0], qs_num[\"DSBusiness\"][1]))\n",
    "ds_business_aggr = percentage_row(ds_business_aggr)\n",
    "\n",
    "cols = ds_business_aggr.columns.tolist()\n",
    "\n",
    "cols = ['cluster',\n",
    " 'No (we do not use ML methods)',\n",
    " 'We use ML methods for generating insights (but do not put working models into production)',\n",
    " 'We are exploring ML methods (and may one day put a model into production)',\n",
    " 'We recently started using ML methods (i.e., models in production for less than 2 years)',\n",
    " 'We have well established ML methods (i.e., models in production for more than 2 years)']\n",
    "\n",
    "ds_business_aggr = ds_business_aggr[cols]\n",
    "\n",
    "ds_business_aggr = ds_business_aggr.loc[ds_business_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(ds_business_aggr, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a higher share of data science team, it's no doubt that C2 has also the highest share of participants that their employer deployed or adopted ML methods in the business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workact_aggr = cluster_aggr(binary_data, range(qs_num[\"WorkAct\"][0], qs_num[\"WorkAct\"][1]))\n",
    "workact_aggr = percentage_row(workact_aggr)\n",
    "\n",
    "cols = workact_aggr.columns.tolist()\n",
    "\"\"\"\n",
    "cols = ['cluster',\n",
    " 'No (we do not use ML methods)',\n",
    " 'We use ML methods for generating insights (but do not put working models into production)',\n",
    " 'We are exploring ML methods (and may one day put a model into production)',\n",
    " 'We recently started using ML methods (i.e., models in production for less than 2 years)',\n",
    " 'We have well established ML methods (i.e., models in production for more than 2 years)']\n",
    "\"\"\"\n",
    "workact_aggr = workact_aggr[cols]\n",
    "\n",
    "workact_aggr = workact_aggr.loc[workact_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(workact_aggr, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a strong background as data scientists in Cluster 2, it explains why they have higher shares of participants working at ML related activites. \n",
    "\n",
    "- Build prototypes to explore applying machine learning to new areas\n",
    "\n",
    "- Build and/or run a machine learning service that operationally improves my product or workflows\n",
    "\n",
    "- Experimentation and iteration to improve existing ML models\n",
    "\n",
    "- Do research that advances the state of the art of machine learning\n",
    "\n",
    "Interestingly, C0 has a higher share of participants working at \n",
    "\n",
    "- Analyze and understand data to influence product or business decisions\n",
    "\n",
    "- None of these activities are an important part of my role at work\n",
    "\n",
    "- Other\n",
    "\n",
    "It could be they have a relatively higher participants working as business analyst / data analysts or other fields. They work closely with Data Scientists and have strong interest in this area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compensation_aggr = cluster_aggr(binary_data, range(qs_num[\"Compensation\"][0], qs_num[\"Compensation\"][1]))\n",
    "compensation_aggr = percentage_row(compensation_aggr)\n",
    "\n",
    "cols = compensation_aggr.columns.tolist()\n",
    "cols = ['cluster',\n",
    " '$0-999',\n",
    " '1,000-1,999',\n",
    " '2,000-2,999',\n",
    " '3,000-3,999',\n",
    " '4,000-4,999',\n",
    " '5,000-7,499',\n",
    " '7,500-9,999',\n",
    " '10,000-14,999',\n",
    " '15,000-19,999',\n",
    " '20,000-24,999',\n",
    " '25,000-29,999',\n",
    " '30,000-39,999',\n",
    " '40,000-49,999',\n",
    " '50,000-59,999',\n",
    " '60,000-69,999',\n",
    " '70,000-79,999',\n",
    " '80,000-89,999',\n",
    " '90,000-99,999',\n",
    " '100,000-124,999',\n",
    " '125,000-149,999',\n",
    " '150,000-199,999',\n",
    " '200,000-249,999',\n",
    " '250,000-299,999',\n",
    " '300,000-499,999',\n",
    " '$500,000-999,999',\n",
    " '>$1,000,000']\n",
    "\n",
    "compensation_aggr = compensation_aggr[cols]\n",
    "\n",
    "compensation_aggr = compensation_aggr.loc[compensation_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(compensation_aggr, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 has higher percentage of participants earning at the higher end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invest_ds_aggr = cluster_aggr(binary_data, range(qs_num[\"InvestDS\"][0], qs_num[\"InvestDS\"][1]))\n",
    "invest_ds_aggr = percentage_row(invest_ds_aggr)\n",
    "\n",
    "cols = invest_ds_aggr.columns.tolist()\n",
    "cols = ['cluster',\n",
    " '$0 ($USD)',\n",
    " '$1-$99',\n",
    " '$100-$999',\n",
    " '$1000-$9,999',\n",
    " '$10,000-$99,999',\n",
    " '$100,000 or more ($USD)']\n",
    "\n",
    "invest_ds_aggr = invest_ds_aggr[cols]\n",
    "\n",
    "invest_ds_aggr = invest_ds_aggr.loc[invest_ds_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(invest_ds_aggr, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 has intested a lot more than any other cluster. C1 has the lowest, could be they are most likely students or unemployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codeExp_aggr = cluster_aggr(binary_data, range(qs_num[\"CodeExp\"][0], qs_num[\"CodeExp\"][1]))\n",
    "codeExp_aggr = percentage_row(codeExp_aggr)\n",
    "\n",
    "cols = codeExp_aggr.columns.tolist()\n",
    "cols = ['cluster',\n",
    " 'I have never written code',\n",
    " '< 1 years',\n",
    " '1-3 years',\n",
    " '3-5 years',\n",
    " '5-10 years',\n",
    " '10-20 years',\n",
    " '20+ years']\n",
    "\n",
    "codeExp_aggr = codeExp_aggr[cols]\n",
    "\n",
    "codeExp_aggr = codeExp_aggr.loc[codeExp_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(codeExp_aggr, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 participants have coding experience more or less, it has the highest percentages of participants coding for over 5 years. On the other hand, C1 have fewer share of participants have no coding experience than the C0 and has a relatively higher share than the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProgLangReg_aggr = cluster_aggr(binary_data, range(qs_num[\"ProgLangReg\"][0], qs_num[\"ProgLangReg\"][1]))\n",
    "ProgLangReg_aggr = percentage_row(ProgLangReg_aggr)\n",
    "top_ProgLangReg_aggr_rank = rank_total(ProgLangReg_aggr)\n",
    "\n",
    "top_ProgLangReg_aggr_rank_cols = ['cluster']\n",
    "for col in top_ProgLangReg_aggr_rank.columns:\n",
    "    top_ProgLangReg_aggr_rank_cols.append(col)\n",
    "    \n",
    "cols = ProgLangReg_aggr.columns\n",
    "\n",
    "ProgLangReg_aggr = ProgLangReg_aggr.loc[ProgLangReg_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(ProgLangReg_aggr[top_ProgLangReg_aggr_rank_cols[:]], cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is the most popular regular programming language, followed by SQL. The third regular programming languages differ that Cluster 2 favors R more, while C1 and C0 prefers C++. It could be the higher share of DS participants allocated in C2 and they are consumers of the R language for statistical modeling. On the other hand, as the role question suggested, software engineers take a noticeable share in C0 and C1 and they use C++ as regular language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProgLangRec_aggr = cluster_aggr(binary_data, range(qs_num[\"ProgLangRec\"][0], qs_num[\"ProgLangRec\"][1]))\n",
    "ProgLangRec_aggr = percentage_row(ProgLangRec_aggr)\n",
    "top_ProgLangRec_aggr_rank = rank_total(ProgLangRec_aggr)\n",
    "\n",
    "top_ProgLangRec_aggr_rank_cols = ['cluster']\n",
    "for col in top_ProgLangRec_aggr_rank.columns:\n",
    "    top_ProgLangRec_aggr_rank_cols.append(col)\n",
    "    \n",
    "cols = ProgLangRec_aggr.columns\n",
    "\n",
    "ProgLangRec_aggr = ProgLangRec_aggr.loc[ProgLangRec_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(ProgLangRec_aggr[top_ProgLangRec_aggr_rank_cols[:]], cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No doubtedly, Python is the most popular language in the recommendation. But interestingly, even though C0 participants are the least group using R as regular language, they suggest it as the most recommending one, which indicates they are aware that this language is a must if you want to understand more of what data scientists do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLmethd_aggr = cluster_aggr(binary_data, range(qs_num[\"MLmethd\"][0], qs_num[\"MLmethd\"][1]))\n",
    "MLmethd_aggr = percentage_row(MLmethd_aggr)\n",
    "\n",
    "cols = MLmethd_aggr.columns.tolist()\n",
    "cols = ['cluster',\n",
    " 'I do not use machine learning methods',\n",
    " 'Under 1 year',\n",
    " '1-2 years',\n",
    " '2-3 years',\n",
    " '3-4 years',\n",
    " '4-5 years',\n",
    " '5-10 years',\n",
    " '10-20 years',\n",
    " '20 or more years']\n",
    "\n",
    "MLmethd_aggr = MLmethd_aggr[cols]\n",
    "\n",
    "MLmethd_aggr = MLmethd_aggr.loc[MLmethd_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(MLmethd_aggr, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C0 has a lot more participants who don't use ML methods at all, 3 times and 30 times as C1 and C2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VisualLib_aggr = cluster_aggr(binary_data, range(qs_num[\"VisualLib\"][0], qs_num[\"VisualLib\"][1]))\n",
    "VisualLib_aggr = percentage_row(VisualLib_aggr)\n",
    "top_VisualLib_rank = rank_total(VisualLib_aggr)\n",
    "\n",
    "top_VisualLib_aggr_rank_cols = ['cluster']\n",
    "for col in top_VisualLib_rank.columns:\n",
    "    top_VisualLib_aggr_rank_cols.append(col)\n",
    "    \n",
    "cols = VisualLib_aggr.columns\n",
    "\n",
    "VisualLib_aggr = VisualLib_aggr.loc[VisualLib_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(VisualLib_aggr[top_VisualLib_aggr_rank_cols[:]], cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_cluster(VisualLib_aggr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 cluster has a wider option in visualization tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLframe_aggr = cluster_aggr(binary_data, range(qs_num[\"MLframe\"][0], qs_num[\"MLframe\"][1]))\n",
    "MLframe_aggr = percentage_row(MLframe_aggr)\n",
    "top_MLframe_rank = rank_total(MLframe_aggr)\n",
    "\n",
    "top_MLframe_aggr_rank_cols = ['cluster']\n",
    "for col in top_MLframe_rank.columns:\n",
    "    top_MLframe_aggr_rank_cols.append(col)\n",
    "    \n",
    "cols = MLframe_aggr.columns\n",
    "\n",
    "MLframe_aggr = MLframe_aggr.loc[MLframe_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(MLframe_aggr[top_MLframe_aggr_rank_cols[:]], cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 cluster has a wider option in ML framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLalgorithm_aggr = cluster_aggr(binary_data, range(qs_num[\"MLalgorithm\"][0], qs_num[\"MLalgorithm\"][1]))\n",
    "MLalgorithm_aggr = percentage_row(MLalgorithm_aggr)\n",
    "top_MLalgorithm_rank = rank_total(MLalgorithm_aggr)\n",
    "\n",
    "top_MLalgorithm_aggr_rank_cols = ['cluster']\n",
    "for col in top_MLalgorithm_rank.columns:\n",
    "    top_MLalgorithm_aggr_rank_cols.append(col)\n",
    "    \n",
    "cols = MLalgorithm_aggr.columns\n",
    "\n",
    "MLalgorithm_aggr = MLalgorithm_aggr.loc[MLalgorithm_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(MLalgorithm_aggr[top_MLalgorithm_aggr_rank_cols[:]], cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_cluster(MLalgorithm_aggr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 cluster has a wider option in ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CompVis_aggr = cluster_aggr(binary_data, range(qs_num[\"CompVis\"][0], qs_num[\"CompVis\"][1]))\n",
    "CompVis_aggr = percentage_row(CompVis_aggr)\n",
    "top_CompVis_rank = rank_total(CompVis_aggr)\n",
    "\n",
    "top_CompVis_aggr_rank_cols = ['cluster']\n",
    "for col in top_CompVis_rank.columns:\n",
    "    top_CompVis_aggr_rank_cols.append(col)\n",
    "    \n",
    "cols = CompVis_aggr.columns\n",
    "\n",
    "CompVis_aggr = CompVis_aggr.loc[CompVis_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(CompVis_aggr[top_CompVis_aggr_rank_cols[:]], cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1 and C2 have similar level of computer vision interests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP_aggr = cluster_aggr(binary_data, range(qs_num[\"NLP\"][0], qs_num[\"NLP\"][1]))\n",
    "NLP_aggr = percentage_row(NLP_aggr)\n",
    "top_NLP_rank = rank_total(NLP_aggr)\n",
    "\n",
    "top_NLP_aggr_rank_cols = ['cluster']\n",
    "for col in top_NLP_rank.columns:\n",
    "    top_NLP_aggr_rank_cols.append(col)\n",
    "    \n",
    "cols = NLP_aggr.columns\n",
    "\n",
    "NLP_aggr = NLP_aggr.loc[NLP_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(NLP_aggr[top_NLP_aggr_rank_cols[:]], cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_cluster(NLP_aggr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 and C1 have far more shares of participants using NLP methods than C0. C2 has clearly more than C1 in Contextualized embeddings (ELMo, CoVe) and Transformer language models (GPT-3, BERT, XLnet, etc) which are less popular methods overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PubShare_aggr = cluster_aggr(binary_data, range(qs_num[\"PubShare\"][0], qs_num[\"PubShare\"][1]))\n",
    "PubShare_aggr = percentage_row(PubShare_aggr)\n",
    "top_PubShare_rank = rank_total(PubShare_aggr)\n",
    "\n",
    "top_PubShare_aggr_rank_cols = ['cluster']\n",
    "for col in top_PubShare_rank.columns:\n",
    "    top_PubShare_aggr_rank_cols.append(col)\n",
    "    \n",
    "cols = PubShare_aggr.columns\n",
    "\n",
    "PubShare_aggr = PubShare_aggr.loc[PubShare_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(PubShare_aggr[top_PubShare_aggr_rank_cols[:]], cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_cluster(PubShare_aggr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 has a very high willing to share their work and knowledge publicly. However, the are a bit less likely to share on Kaggle than C1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Courses_aggr = cluster_aggr(binary_data, range(qs_num[\"Courses\"][0], qs_num[\"Courses\"][1]))\n",
    "Courses_aggr = percentage_row(Courses_aggr)\n",
    "top_Courses_rank = rank_total(Courses_aggr)\n",
    "\n",
    "top_Courses_aggr_rank_cols = ['cluster']\n",
    "for col in top_Courses_rank.columns:\n",
    "    top_Courses_aggr_rank_cols.append(col)\n",
    "    \n",
    "cols = Courses_aggr.columns\n",
    "\n",
    "Courses_aggr = Courses_aggr.loc[Courses_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(Courses_aggr[top_Courses_aggr_rank_cols[:]], cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_cluster(Courses_aggr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1 and C2 are more likely to choose online courses than C0. Compared with C1, C2 participants are less into Kaggle courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FavMedia_aggr = cluster_aggr(binary_data, range(qs_num[\"FavMedia\"][0], qs_num[\"FavMedia\"][1]))\n",
    "FavMedia_aggr = percentage_row(FavMedia_aggr)\n",
    "top_FavMedia_rank = rank_total(FavMedia_aggr)\n",
    "\n",
    "top_FavMedia_aggr_rank_cols = ['cluster']\n",
    "for col in top_FavMedia_rank.columns:\n",
    "    top_FavMedia_aggr_rank_cols.append(col)\n",
    "    \n",
    "cols = FavMedia_aggr.columns\n",
    "\n",
    "FavMedia_aggr = FavMedia_aggr.loc[FavMedia_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(FavMedia_aggr[top_FavMedia_aggr_rank_cols[:]], cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1 and C2 are more likely to choose data science media than C0. C2 participants are more into Email, newsletter, podcasts and journals. C0 and C1 are more into Youtube and Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDE_aggr = cluster_aggr(binary_data, range(qs_num[\"IDE\"][0], qs_num[\"IDE\"][1]))\n",
    "IDE_aggr = percentage_row(IDE_aggr)\n",
    "top_IDE_rank = rank_total(IDE_aggr)\n",
    "\n",
    "top_IDE_aggr_rank_cols = ['cluster']\n",
    "for col in top_IDE_rank.columns:\n",
    "    top_IDE_aggr_rank_cols.append(col)\n",
    "    \n",
    "cols = IDE_aggr.columns\n",
    "\n",
    "IDE_aggr = IDE_aggr.loc[IDE_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(IDE_aggr[top_IDE_aggr_rank_cols[:]], cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the main difference among each IDE? - Maybe closely with the knowledge and skillset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HostNotebook_aggr = cluster_aggr(binary_data, range(qs_num[\"HostNotebook\"][0], qs_num[\"HostNotebook\"][1]))\n",
    "HostNotebook_aggr = percentage_row(HostNotebook_aggr)\n",
    "top_HostNotebook_rank = rank_total(HostNotebook_aggr)\n",
    "\n",
    "top_HostNotebook_aggr_rank_cols = ['cluster']\n",
    "for col in top_HostNotebook_rank.columns:\n",
    "    top_HostNotebook_aggr_rank_cols.append(col)\n",
    "    \n",
    "cols = HostNotebook_aggr.columns\n",
    "\n",
    "HostNotebook_aggr = HostNotebook_aggr.loc[HostNotebook_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(HostNotebook_aggr[top_HostNotebook_aggr_rank_cols[:]], cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "company (work) related?\n",
    "\n",
    "needs to figure out the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CompPlatMost_aggr = cluster_aggr(binary_data, range(qs_num[\"CompPlatMost\"][0], qs_num[\"CompPlatMost\"][1]))\n",
    "CompPlatMost_aggr = percentage_row(CompPlatMost_aggr)\n",
    "top_CompPlatMost_rank = rank_total(CompPlatMost_aggr)\n",
    "\n",
    "top_CompPlatMost_aggr_rank_cols = ['cluster']\n",
    "for col in top_CompPlatMost_rank.columns:\n",
    "    top_CompPlatMost_aggr_rank_cols.append(col)\n",
    "    \n",
    "cols = CompPlatMost_aggr.columns\n",
    "\n",
    "CompPlatMost_aggr = CompPlatMost_aggr.loc[CompPlatMost_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(CompPlatMost_aggr[top_CompPlatMost_aggr_rank_cols[:]], cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 have more in cloud computing platform and deep learning workstation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HardwareReg_aggr = cluster_aggr(binary_data, range(qs_num[\"HardwareReg\"][0], qs_num[\"HardwareReg\"][1]))\n",
    "HardwareReg_aggr = percentage_row(HardwareReg_aggr)\n",
    "top_HardwareReg_rank = rank_total(HardwareReg_aggr)\n",
    "\n",
    "top_HardwareReg_aggr_rank_cols = ['cluster']\n",
    "for col in top_HardwareReg_rank.columns:\n",
    "    top_HardwareReg_aggr_rank_cols.append(col)\n",
    "    \n",
    "cols = HardwareReg_aggr.columns\n",
    "\n",
    "HardwareReg_aggr = HardwareReg_aggr.loc[HardwareReg_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(HardwareReg_aggr[top_HardwareReg_aggr_rank_cols[:]], cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 have more in NVIDIA GPU and Google Cloud TPU than C1 or C0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPUtimes_aggr = cluster_aggr(binary_data, range(qs_num[\"TPUtimes\"][0], qs_num[\"TPUtimes\"][1]))\n",
    "TPUtimes_aggr = percentage_row(TPUtimes_aggr)\n",
    "\n",
    "cols = TPUtimes_aggr.columns.tolist()\n",
    "cols = ['cluster',\n",
    "        'Never',\n",
    "        'Once',\n",
    "        '2-5 times',\n",
    "        '6-25 times',\n",
    "        'More than 25 times']\n",
    "\n",
    "TPUtimes_aggr = TPUtimes_aggr[cols]\n",
    "\n",
    "TPUtimes_aggr = TPUtimes_aggr.loc[TPUtimes_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(TPUtimes_aggr, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More times of C2 than C0 or C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CldCompPltReg_aggr = cluster_aggr(binary_data, range(qs_num[\"CldCompPltReg\"][0], qs_num[\"CldCompPltReg\"][1]))\n",
    "CldCompPltReg_aggr = percentage_row(CldCompPltReg_aggr)\n",
    "top_CldCompPltReg_rank = rank_total(CldCompPltReg_aggr)\n",
    "\n",
    "top_CldCompPltReg_aggr_rank_cols = ['cluster']\n",
    "for col in top_CldCompPltReg_rank.columns:\n",
    "    top_CldCompPltReg_aggr_rank_cols.append(col)\n",
    "    \n",
    "cols = CldCompPltReg_aggr.columns\n",
    "\n",
    "CldCompPltReg_aggr = CldCompPltReg_aggr.loc[CldCompPltReg_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(CldCompPltReg_aggr[top_CldCompPltReg_aggr_rank_cols[:]], cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS,Micosoft Azure, and GCP - C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CldCompPltBstExp_aggr = cluster_aggr(binary_data, range(qs_num[\"CldCompPltBstExp\"][0], qs_num[\"CldCompPltBstExp\"][1]))\n",
    "CldCompPltBstExp_aggr = percentage_row(CldCompPltBstExp_aggr)\n",
    "top_CldCompPltBstExp_rank = rank_total(CldCompPltBstExp_aggr)\n",
    "\n",
    "top_CldCompPltBstExp_aggr_rank_cols = ['cluster']\n",
    "for col in top_CldCompPltBstExp_rank.columns:\n",
    "    top_CldCompPltBstExp_aggr_rank_cols.append(col)\n",
    "    \n",
    "cols = CldCompPltBstExp_aggr.columns\n",
    "\n",
    "CldCompPltBstExp_aggr = CldCompPltBstExp_aggr.loc[CldCompPltBstExp_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(CldCompPltBstExp_aggr[top_CldCompPltBstExp_aggr_rank_cols[:]], cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CldCompProdReg_aggr = cluster_aggr(binary_data, range(qs_num[\"CldCompProdReg\"][0], qs_num[\"CldCompProdReg\"][1]))\n",
    "CldCompProdReg_aggr = percentage_row(CldCompProdReg_aggr)\n",
    "CldCompProdReg_rank = rank_total(CldCompProdReg_aggr)\n",
    "\n",
    "top_CldCompProdReg_aggr_rank_cols = ['cluster']\n",
    "for col in CldCompProdReg_rank.columns:\n",
    "    top_CldCompProdReg_aggr_rank_cols.append(col)\n",
    "    \n",
    "cols = CldCompProdReg_aggr.columns\n",
    "\n",
    "CldCompProdReg_aggr = CldCompProdReg_aggr.loc[CldCompProdReg_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(CldCompProdReg_aggr[top_CldCompProdReg_aggr_rank_cols[:]], cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataStoreProdReg_aggr = cluster_aggr(binary_data, range(qs_num[\"DataStoreProdReg\"][0], qs_num[\"DataStoreProdReg\"][1]))\n",
    "DataStoreProdReg_aggr = percentage_row(DataStoreProdReg_aggr)\n",
    "DataStoreProdReg_rank = rank_total(DataStoreProdReg_aggr)\n",
    "\n",
    "top_DataStoreProdReg_aggr_rank_cols = ['cluster']\n",
    "for col in DataStoreProdReg_rank.columns:\n",
    "    top_DataStoreProdReg_aggr_rank_cols.append(col)\n",
    "    \n",
    "cols = DataStoreProdReg_aggr.columns\n",
    "\n",
    "DataStoreProdReg_aggr = DataStoreProdReg_aggr.loc[DataStoreProdReg_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(DataStoreProdReg_aggr[top_DataStoreProdReg_aggr_rank_cols[:]], cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ManageMLProdReg_aggr = cluster_aggr(binary_data, range(qs_num[\"ManageMLProdReg\"][0], qs_num[\"ManageMLProdReg\"][1]))\n",
    "ManageMLProdReg_aggr = percentage_row(ManageMLProdReg_aggr)\n",
    "ManageMLProdReg_rank = rank_total(ManageMLProdReg_aggr)\n",
    "\n",
    "top_ManageMLProdReg_aggr_rank_cols = ['cluster']\n",
    "for col in ManageMLProdReg_rank.columns:\n",
    "    top_ManageMLProdReg_aggr_rank_cols.append(col)\n",
    "    \n",
    "cols = ManageMLProdReg_aggr.columns\n",
    "\n",
    "ManageMLProdReg_aggr = ManageMLProdReg_aggr.loc[ManageMLProdReg_aggr['cluster'].isin([0,1,2])]\n",
    "\n",
    "plot_bar_perc(ManageMLProdReg_aggr[top_ManageMLProdReg_aggr_rank_cols[:]], cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"BigDataProdReg\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 has more and wider options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"BigDataProdMost\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks C1 is more centralized in the options of most big data products. C2 has more in postgress while C0 has more in microsoft SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"CldCompProdReg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 has more. C0 has same level of number in Microsoft Azure Virtual Machines as C2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"IntegenceReg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 has more and wider option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"IntegenceMost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1 likes Power BI more while C0 and C2 likes Tableau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"IsAutoML\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First category that over 40% of the C2 participants chose none. \n",
    "\n",
    "That might explain why they want to learn it in the next 2 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"AutoMLReg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C2 has the highest share chosen None. \n",
    "\n",
    "Is this the next future?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"MLexperiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A category which is less chosen. C2 have over half participants chose 0 and over 80% for any other cluster.\n",
    "\n",
    "TensorBoard and MLflow are most popular ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"PrimaryTool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1 and C2 learned how to code at the very beginning when they start to step into the statistical world.\n",
    "\n",
    "That explains the coding experience.\n",
    "\n",
    "C0 starts with statistics software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"CldCompPltNxt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1 is the most ambicious and C2 is the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"CldCompProdNxt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1 is the most ambicious and C2 is the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data unavailable\n",
    "# plot_bar_rank(binary_data, \"DataStoreProdNxt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No data available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"ManageMLProdNxt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1 is the most ambicious and C2 is the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"BigDataProdNxt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1 is the most ambicious and C2 is the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"IntegenceNxt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1 is the most ambicious and C2 is the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"AutoMLCatNxt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "too hard for C0. C1 is the most ambicious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"AutoMLProdNxt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1 is the most ambicious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_rank(binary_data, \"MLexperimentNxt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C1 is the most ambicious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a correlation matrix between each pair of column\n",
    "binary_data_corr = binary_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requiment\n",
    "Python libraries are required to run this notebook\n",
    "\n",
    "os/numpy/pandas/matplotlib/sklearn/pyarrow/plotly\n",
    "\n",
    "## Acknowledge\n",
    "The dataset for building this model is coming from Kaggle. Kudos to the Kaggle community!\n",
    "\n",
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "By adopting K-means cluster analysis, I find there are three types of participants - C0, C1 and C2, depending on \n",
    "\n",
    "the answers they choose to the survey. I'd categorise questions into five groups\n",
    "\n",
    "1. Demographics\n",
    "    Q1_Age, Q2_Gender, Q3_Country, Q4_Highest_Education\n",
    "    \n",
    "2. Profession\n",
    "    Q5_Role, Q19_industry, Q20_size_employer, Q21_size_ds, \n",
    "    Q22_ds_business, Q23_work_activity, Q24_compensation, Q25_invest\n",
    "    \n",
    "3. Tools\n",
    "    Q9_IDE, Q10_hosted_notebook_product, Q11_computing_platform, Q12_specialized_hardware, Q13_TPU_times, \n",
    "    Q26_cloud_computing_platform_regular, Q27_cloud_computing_platform_exp, Q28_cloud_computing_product,\n",
    "    Q29_data_storage_product, Q30_big_data_prod_regular, Q31_big_data_prod_most, Q32_intelligence_regular,\n",
    "    Q33_intelligence_most, Q34_is_use_auto_ml, Q35_auto_ml_regular, Q36_ml_experiment, Q39_primary_tool\n",
    "    \n",
    "4. Knowledge and Skillset\n",
    "    Q6_yr_coding, Q7_programming_language_regular, Q8_programming_language_recommend, Q14_visualization_lib,\n",
    "    Q15_yr_machine_learning, Q16_machine_learning_framework, Q16_machine_learning_algorithm, Q17_computer_vision_method,\n",
    "    Q18_nlp_method, Q37_pub_share, Q38_courses, Q40_fav_media, \n",
    "    \n",
    "5. Development\n",
    "    Q11_computing_platform_2yr, Q28_cloud_computing_product_2yr, Q29_data_storage_product_2yr, Q30_big_data_prod_2yr,\n",
    "    _ml_prod_2yr, Q32_intelligence_2yr, Q35_auto_ml_2yr, Q36_ml_experiment_2yr\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "col\n",
    "\n",
    "0 : 11 - age\n",
    "c0 and c1 younger groups 18-21, 22-24, 25-29\n",
    "\n",
    "c2 30-34 highest share\n",
    "\n",
    "c1 youngest\n",
    "\n",
    "11 : 16 - gender\n",
    "all male dominant\n",
    "for cluster 2, the male is the highest over 85%\n",
    "\n",
    "\n",
    "16 : 82 - country\n",
    "Developing countries, China, India and Russia have lower shares in cluster 2,\n",
    "while it is the opposite in western developed countries\n",
    "\n",
    "82 : 89 - education\n",
    "c2 has higher share of Doctoral degree than 1 and 2\n",
    "\n",
    "89 : 104 - employment\n",
    "\n",
    "c1 highest unemployed and students, c2 lowest\n",
    "\n",
    "c0,1 students high\n",
    "\n",
    "c2 ds 29%\n",
    "\n",
    "c0,1 da, sfe\n",
    "\n",
    "104 : 111 - coding exp\n",
    "c2 highest\n",
    "\n",
    "c0 least experienced in coding\n",
    "\n",
    "111 : 125 - What programming languages do you use on a regular basis?\n",
    "Python very close among the three clusters\n",
    "\n",
    "c0,1 higher at C and C++\n",
    "\n",
    "c2, higher at R and SQL\n",
    "\n",
    "125 : 138 - recommend language for new ds\n",
    "\n",
    "c0 has a same level with c2 that use Python as basis language in regular\n",
    "however, c0 has 5% lower in recommeding DS to leanr Py than c2. \n",
    "R and SQL actually high in shares in C0, considering theri lower usage of both in regular basis\n",
    "\n",
    "138 : 151 - Which of the following integrated development environments (IDE's) do you use on a regular basis? \n",
    "\n",
    "Jupyter notebook among three highest\n",
    "\n",
    "Vim / Emacs c2 has double in shares, absolute value lowest though\n",
    "\n",
    "c0 highest VS studio and Pycharm\n",
    "\n",
    "151 : 169 Which of the following hosted notebook products do you use on a regular basis?\n",
    "\n",
    "c0 33% no hostednoted book\n",
    "\n",
    "169 : 175 Q11_What type of computing platform do you use most often for your data science projects?\n",
    "\n",
    "c2 cloud computing and deep learning work station higher\n",
    "\n",
    "c1 has highest share in laptop, 2% higher than c0\n",
    "\n",
    "\n",
    "175 : 182 Q12_Part_1_Which types of specialized hardware do you use on a regular basis?\n",
    "\n",
    "C2 have far more hardware than 1 and 0\n",
    "\n",
    "182 : 187 Q13_Approximately how many times have you used a TPU (tensor processing unit)?\n",
    "\n",
    "C2 used it more often than any of the other two\n",
    "\n",
    "187 : 200 What data visualization libraries or tools do you use on a regular basis?\n",
    "\n",
    "C2 has a wider range of visualization options e.g. shiny D3\n",
    "\n",
    "200 : 209 - machine learning methods\n",
    "\n",
    "Over 1 year is a critical period for ML learning\n",
    "\n",
    "210 : 227 - Which of the following machine learning frameworks do you use on a regular basis?\n",
    "\n",
    "C2 has a wider range of framework options\n",
    "\n",
    "227 : 239 - Which of the following ML algorithms do you use on a regular basis?\n",
    "\n",
    "C2 wider range\n",
    "\n",
    "239 : 246 - Which categories of computer vision methods do you use on a regular basis?\n",
    "\n",
    "c1 and c2 are very similar\n",
    "\n",
    "246 : 253 - Which of the following natural language processing (NLP) methods do you use on a regular basis?\n",
    "\n",
    "c2 has wider range\n",
    "\n",
    "253 : 272 - industry\n",
    "c0 and 1 higher in academics and education\n",
    "\n",
    "c2 computer / tech / online business sales\n",
    "\n",
    "272 : 278 - size of employer\n",
    "c2 highest in largest employers\n",
    "\n",
    "278 : 287 - Approximately how many individuals are responsible for data science workloads at your place of business?\n",
    "\n",
    "c2 has the highest share in highest number\n",
    "\n",
    "287 : 292 - Does your current employer incorporate machine learning methods into their business?\n",
    "\n",
    "C2 highest in model in production\n",
    "\n",
    "293 : 301 - Select any activities that make up an important part of your role at work\n",
    "\n",
    "0 and 1 are more faciliate\n",
    "\n",
    "2 is more advanced and automated?\n",
    "\n",
    "301 : 328 - yearly compensation\n",
    "\n",
    "c2 highest, then c0 , c1 lastly\n",
    "\n",
    "328 : 334 - Approximately how much money have you (or your team) spent on machine learning and/or cloud computing services at home (or at work) in the past 5 years (approximate $USD)? \n",
    "\n",
    "c2 highest, then 0 and 1 almost none\n",
    "\n",
    "334 : 347 - Which of the following cloud computing platforms do you use on a regular basis?\n",
    "\n",
    "c2 has highest in almost all popular platforms\n",
    "\n",
    "347 : 360 - Of the cloud platforms that you are familiar with, which has the best developer experience\n",
    "\n",
    "C2 likes AWS while C1 likes Google Cloud platform\n",
    "\n",
    "360 : 365 - Do you use any of the following cloud computing products on a regular basis?\n",
    "\n",
    "c2 Amazon Elastic Compute Cloud  \n",
    "\n",
    "c2 and c0 similar at Microsoft Azure Virtual Machines\n",
    "\n",
    "365 : 373 - Do you use any of the following data storage products on a regular basis?\n",
    "\n",
    "same trend as platform and product\n",
    "\n",
    "373 : 383 - Do you use any of the following managed machine learning products on a regular basis?\n",
    "\n",
    "\n",
    "383 : 405 - Which of the following big data products (relational databases, data warehouses, data lakes, or similar) do you use on a regular basis?\n",
    "My SQL most popular among the three\n",
    "\n",
    "405 : 425 - Which of the following big data products (relational database, data warehouse, data lake, or similar) do you use most often?\n",
    "\n",
    "MySQL for all popular\n",
    "Microsoft SQL server C1\n",
    "PostgreSQL C2\n",
    "\n",
    "425 : 443 - Which of the following business intelligence tools do you use on a regular basis?\n",
    "C2 - C1 - C0\n",
    "tableau\n",
    "microsoft power bi\n",
    "\n",
    "443 : 459 - Which of the following business intelligence tools do you use most often?\n",
    "Microsoft Power BI\n",
    "and Tableau\n",
    "\n",
    "C2 has more flexiblity at the most often used bi tools\n",
    "\n",
    "459 : 467 - Do you use any automated machine learning tools (or partial AutoML tools) on a regular basis?\n",
    "\n",
    "c2 highest in each category\n",
    "\n",
    "model selection, augmented data and parameter tuning\n",
    "\n",
    "467 : 475 - Which of the following automated machine learning tools (or partial AutoML tools) do you use on a regular basis?\n",
    "C2 has a lot more share choosing None\n",
    "\n",
    "475 : 487 - Do you use any tools to help manage machine learning experiments?\n",
    "\n",
    "Tensorflow, Weights and bias\n",
    "\n",
    "487 : 497 - Where do you publicly share your data analysis or machine learning applications?\n",
    "C2 wins at most categories\n",
    "\n",
    "Github, Kaggle, Colab\n",
    "\n",
    "497 : 510 - On which platforms have you begun or completed data science courses?\n",
    "\n",
    "Platform certificats, udacity and coursera - c2\n",
    "\n",
    "C2 don't really like Kaggle courses\n",
    "\n",
    "510 : 516 - What is the primary tool that you use at work or school to analyze data?\n",
    "C2 and 1 high in Local development environments (RStudio, JupyterLab, etc.\n",
    "C0 basic statistic software\n",
    "\n",
    "516 : 528 - Who/what are your favorite media sources that report on data science topics?\n",
    "\n",
    "C2 podcast, journal, slack community and blogs\n",
    "\n",
    "528 : 540 - Which of the following cloud computing platforms do you hope to become more familiar with in the next 2 years?\n",
    "C2 highest in None!\n",
    "\n",
    "540 : 545 - In the next 2 years, do you hope to become more familiar with any of these specific cloud computing products?\n",
    "C2 highest in None!\n",
    "\n",
    "545 : 553 - In the next 2 years, do you hope to become more familiar with any of these specific data storage products?\n",
    "All NaN, data issue? No, they are all None actually.\n",
    "\n",
    "553 : 563 - In the next 2 years, do you hope to become more familiar with any of these managed machine learning products?\n",
    "C2 highest in None!\n",
    "\n",
    "563 : 584 - Which of the following big data products (relational databases, data warehouses, data lakes, or similar) do you hope to become more familiar with in the next 2 years?\n",
    "Amazon RDS, snowflake for C2\n",
    "C2 highest in None!\n",
    "\n",
    "584 : 601 - hich of the following business intelligence tools do you hope to become more familiar with in the next 2 years?\n",
    "C2 highest in None!\n",
    "\n",
    "601 : 609 - Which categories of automated machine learning tools (or partial AutoML tools) do you hope to become more familiar with in the next 2 years?\n",
    "C2 Automated feature engineering/selection and parameter tuning\n",
    "\n",
    "609 : 617 - Which specific automated machine learning tools (or partial AutoML tools) do you hope to become more familiar with in the next 2 years?\n",
    "C2 highest in Other and None\n",
    "\n",
    "617 : 629 - In the next 2 years, do you hope to become more familiar with any of these tools for managing ML experiments?\n",
    "C2 TensorBoard and MLflow\n",
    "\n",
    "high in None and Other\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a correlation matrix between each pair of column\n",
    "binary_data_corr = binary_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"Q9_Part_4_Which of the following integrated development environments (IDE's) do you use on a regular basis?  (Select all that apply) - Selected Choice -  Visual Studio Code (VSCode) \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_data_corr[[col]].sort_values(by = col,ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_data_corr[['Q7_Part_10_What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Bash']].sort_values(\n",
    "by = 'Q7_Part_10_What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Bash',\n",
    "ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_data_corr[[\"Q9_Part_9_Which of the following integrated development environments (IDE's) do you use on a regular basis?  (Select all that apply) - Selected Choice -   Vim / Emacs  \"]].sort_values(\n",
    "by = \"Q9_Part_9_Which of the following integrated development environments (IDE's) do you use on a regular basis?  (Select all that apply) - Selected Choice -   Vim / Emacs  \",\n",
    "ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_data_corr[[\"Q9_Part_10_Which of the following integrated development environments (IDE's) do you use on a regular basis?  (Select all that apply) - Selected Choice -  MATLAB \"]].sort_values(\n",
    "by = \"Q9_Part_10_Which of the following integrated development environments (IDE's) do you use on a regular basis?  (Select all that apply) - Selected Choice -  MATLAB \",\n",
    "ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "# data = pd.read_csv(\"../input/kaggle-survey-2021/kaggle_survey_2021_responses.csv\", low_memory = False)\n",
    "\n",
    "#data_matrix = binary_data.dot(np.transpose(binary_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
